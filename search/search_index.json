{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/","title":"Sections","text":""},{"location":"api_reference/#components","title":"Components","text":"<ul> <li>AI Classifiers</li> <li>AI Functions</li> <li>AI Models</li> </ul>"},{"location":"api_reference/#settings","title":"Settings","text":"<ul> <li>settings</li> </ul>"},{"location":"api_reference/#utilities","title":"Utilities","text":"<ul> <li><code>asyncio</code></li> <li><code>context</code></li> <li><code>jinja</code></li> <li><code>logging</code></li> <li><code>openai</code></li> <li><code>pydantic</code></li> <li><code>slack</code></li> <li><code>strings</code></li> <li><code>tools</code></li> </ul>"},{"location":"api_reference/settings/","title":"settings","text":""},{"location":"api_reference/settings/#marvin.settings","title":"<code>marvin.settings</code>","text":"<p>Settings for configuring <code>marvin</code>.</p>"},{"location":"api_reference/settings/#marvin.settings--requirements","title":"Requirements","text":"<p>All you need to configure is your OpenAI API key.</p> <p>You can set this in <code>~/.marvin/.env</code> or as an environment variable on your system: <pre><code>MARVIN_OPENAI_API_KEY=sk-...\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.AssistantSettings","title":"<code>AssistantSettings</code>","text":"<p>Settings for the assistant API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default assistant model to use, defaults to <code>gpt-4-1106-preview</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.ChatCompletionSettings","title":"<code>ChatCompletionSettings</code>","text":"<p>Settings for chat completions.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default chat model to use, defaults to <code>gpt-3.5-turbo-1106</code>.</p> Example <p>Set the current chat model to <code>gpt-4-1106-preview</code>: <pre><code>import marvin\n\nmarvin.settings.openai.chat.completions.model = \"gpt-4-1106-preview\"\n\nassert marvin.settings.openai.chat.completions.model == \"gpt-4-1106-preview\"\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.ImageSettings","title":"<code>ImageSettings</code>","text":"<p>Settings for OpenAI's image API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default image model to use, defaults to <code>dall-e-3</code>.</p> <code>size</code> <code>Literal['1024x1024', '1792x1024', '1024x1792']</code> <p>The default image size to use, defaults to <code>1024x1024</code>.</p> <code>response_format</code> <code>Literal['url', 'b64_json']</code> <p>The default response format to use, defaults to <code>url</code>.</p> <code>style</code> <code>Literal['vivid', 'natural']</code> <p>The default style to use, defaults to <code>vivid</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.OpenAISettings","title":"<code>OpenAISettings</code>","text":"<p>Settings for the OpenAI API.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[SecretStr]</code> <p>Your OpenAI API key.</p> <code>organization</code> <code>Optional[str]</code> <p>Your OpenAI organization ID.</p> <code>chat</code> <code>ChatSettings</code> <p>Settings for the chat API.</p> <code>images</code> <code>ImageSettings</code> <p>Settings for the images API.</p> <code>audio</code> <code>AudioSettings</code> <p>Settings for the audio API.</p> <code>assistants</code> <code>AssistantSettings</code> <p>Settings for the assistants API.</p> Example <p>Set the OpenAI API key: <pre><code>import marvin\n\nmarvin.settings.openai.api_key = \"sk-...\"\n\nassert marvin.settings.openai.api_key.get_secret_value() == \"sk-...\"\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.Settings","title":"<code>Settings</code>","text":"<p>Settings for <code>marvin</code>.</p> <p>This is the main settings object for <code>marvin</code>.</p> <p>Attributes:</p> Name Type Description <code>openai</code> <code>OpenAISettings</code> <p>Settings for the OpenAI API.</p> <code>log_level</code> <code>str</code> <p>The log level to use, defaults to <code>DEBUG</code>.</p> Example <p>Set the log level to <code>INFO</code>: <pre><code>import marvin\n\nmarvin.settings.log_level = \"INFO\"\n\nassert marvin.settings.log_level == \"INFO\"\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.SpeechSettings","title":"<code>SpeechSettings</code>","text":"<p>Settings for OpenAI's speech API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default speech model to use, defaults to <code>tts-1-hd</code>.</p> <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The default voice to use, defaults to <code>alloy</code>.</p> <code>response_format</code> <code>Literal['mp3', 'opus', 'aac', 'flac']</code> <p>The default response format to use, defaults to <code>mp3</code>.</p> <code>speed</code> <code>float</code> <p>The default speed to use, defaults to <code>1.0</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.temporary_settings","title":"<code>temporary_settings</code>","text":"<p>Temporarily override Marvin setting values, including nested settings objects.</p> <p>To override nested settings, use <code>__</code> to separate nested attribute names.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>The settings to override, including nested settings.</p> <code>{}</code> Example <p>Temporarily override the OpenAI API key: <pre><code>import marvin\nfrom marvin.settings import temporary_settings\n\n# Override top-level settings\nwith temporary_settings(log_level=\"INFO\"):\n    assert marvin.settings.log_level == \"INFO\"\nassert marvin.settings.log_level == \"DEBUG\"\n\n# Override nested settings\nwith temporary_settings(openai__api_key=\"new-api-key\"):\n    assert marvin.settings.openai.api_key.get_secret_value() == \"new-api-key\"\nassert marvin.settings.openai.api_key.get_secret_value().startswith(\"sk-\")\n</code></pre></p>"},{"location":"api_reference/components/ai_classifier/","title":"ai_classifier","text":""},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier","title":"<code>marvin.components.ai_classifier</code>","text":""},{"location":"api_reference/components/ai_function/","title":"ai_function","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function","title":"<code>marvin.components.ai_function</code>","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.AIFunction","title":"<code>AIFunction</code>","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.AIFunction.amap","title":"<code>amap</code>  <code>async</code>","text":"<p>Map the AI function over a sequence of arguments. Runs concurrently.</p> <p>A <code>map</code> twin method is provided by the <code>expose_sync_method</code> decorator.</p> <p>You can use <code>map</code> or <code>amap</code> synchronously or asynchronously, respectively, regardless of whether the user function is synchronous or asynchronous.</p> <p>Arguments should be provided as if calling the function normally, but each argument must be a list. The function is called once for each item in the list, and the results are returned in a list.</p> <p>For example, fn.map([1, 2]) is equivalent to [fn(1), fn(2)].</p> <p>fn.map([1, 2], x=['a', 'b']) is equivalent to [fn(1, x='a'), fn(2, x='b')].</p>"},{"location":"api_reference/components/ai_model/","title":"ai_model","text":""},{"location":"api_reference/components/ai_model/#marvin.components.ai_model","title":"<code>marvin.components.ai_model</code>","text":""},{"location":"api_reference/utilities/asyncio/","title":"asyncio","text":""},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio","title":"<code>marvin.utilities.asyncio</code>","text":"<p>Utilities for working with asyncio.</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.ExposeSyncMethodsMixin","title":"<code>ExposeSyncMethodsMixin</code>","text":"<p>A mixin that can take functions decorated with <code>expose_sync_method</code> and automatically create synchronous versions.</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.expose_sync_method","title":"<code>expose_sync_method</code>","text":"<p>Decorator that automatically exposes synchronous versions of async methods. Note it doesn't work with classmethods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the synchronous method.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>The decorated function.</p> Example <p>Basic usage: <pre><code>class MyClass(ExposeSyncMethodsMixin):\n\n    @expose_sync_method(\"my_method\")\n    async def my_method_async(self):\n        return 42\n\nmy_instance = MyClass()\nawait my_instance.my_method_async() # returns 42\nmy_instance.my_method()  # returns 42\n</code></pre></p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Runs a synchronous function in an asynchronous manner.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., T]</code> <p>The function to run.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The return value of the function.</p> Example <p>Basic usage: <pre><code>def my_sync_function(x: int) -&gt; int:\n    return x + 1\n\nawait run_async(my_sync_function, 1)\n</code></pre></p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.run_sync","title":"<code>run_sync</code>","text":"<p>Runs a coroutine from a synchronous context, either in the current event loop or in a new one if there is no event loop running. The coroutine will block until it is done. A thread will be spawned to run the event loop if necessary, which allows coroutines to run in environments like Jupyter notebooks where the event loop runs on the main thread.</p> <p>Parameters:</p> Name Type Description Default <code>coroutine</code> <code>Coroutine[Any, Any, T]</code> <p>The coroutine to run.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The return value of the coroutine.</p> Example <p>Basic usage: <pre><code>async def my_async_function(x: int) -&gt; int:\n    return x + 1\n\nrun_sync(my_async_function(1))\n</code></pre></p>"},{"location":"api_reference/utilities/context/","title":"context","text":""},{"location":"api_reference/utilities/context/#marvin.utilities.context","title":"<code>marvin.utilities.context</code>","text":"<p>Module for defining context utilities.</p>"},{"location":"api_reference/utilities/context/#marvin.utilities.context.ScopedContext","title":"<code>ScopedContext</code>","text":"<p><code>ScopedContext</code> provides a context management mechanism using <code>contextvars</code>.</p> <p>This class allows setting and retrieving key-value pairs in a scoped context, which is preserved across asynchronous tasks and threads within the same context.</p> <p>Attributes:</p> Name Type Description <code>_context_storage</code> <code>ContextVar</code> <p>A context variable to store the context data.</p> Example <p>Basic Usage of ScopedContext <pre><code>context = ScopedContext()\nwith context(key=\"value\"):\n    assert context.get(\"key\") == \"value\"\n# Outside the context, the value is no longer available.\nassert context.get(\"key\") is None\n</code></pre></p>"},{"location":"api_reference/utilities/context/#marvin.utilities.context.ScopedContext.get","title":"<code>get</code>","text":"<p>Retrieves the value for a given key from the context.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>The key to retrieve the value for.</p> required <code>default</code> <p>The default value to return if the key is not found.</p> <code>None</code> <p>Returns:</p> Type Description <p>The value for the key, or the default value if the value is not found.</p>"},{"location":"api_reference/utilities/context/#marvin.utilities.context.ScopedContext.set","title":"<code>set</code>","text":"<p>Sets one or more key-value pairs in the context.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Key-value pairs to set in the context.</p> <code>{}</code>"},{"location":"api_reference/utilities/jinja/","title":"jinja","text":""},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja","title":"<code>marvin.utilities.jinja</code>","text":"<p>Module for Jinja utilities.</p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.BaseEnvironment","title":"<code>BaseEnvironment</code>","text":"<p>BaseEnvironment provides a configurable environment for rendering Jinja templates.</p> <p>This class encapsulates a Jinja environment with customizable global functions and template settings, allowing for flexible template rendering.</p> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <p>The Jinja environment for template rendering.</p> <code>globals</code> <code>dict[str, Any]</code> <p>A dictionary of global functions and variables available in templates.</p> Example <p>Basic Usage of BaseEnvironment <pre><code>env = BaseEnvironment()\n\nrendered = env.render(\"Hello, {{ name }}!\", name=\"World\")\nprint(rendered)  # Output: Hello, World!\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.BaseEnvironment.render","title":"<code>render</code>","text":"<p>Renders a given template <code>str</code> or <code>BaseTemplate</code> with provided context.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>Union[str, Template]</code> <p>The template to be rendered.</p> required <code>**kwargs</code> <code>Any</code> <p>Context variables to be passed to the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The rendered template as a string.</p> Example <p>Basic Usage of <code>BaseEnvironment.render</code> <pre><code>from marvin.utilities.jinja import Environment as jinja_env\n\nrendered = jinja_env.render(\"Hello, {{ name }}!\", name=\"World\")\nprint(rendered) # Output: Hello, World!\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.Transcript","title":"<code>Transcript</code>","text":"<p>Transcript is a model representing a conversation involving multiple roles.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The content of the transcript.</p> <code>roles</code> <code>list[str]</code> <p>The roles involved in the transcript.</p> <code>environment</code> <code>BaseEnvironment</code> <p>The jinja environment to use for rendering the transcript.</p> Example <p>Basic Usage of Transcript: <pre><code>from marvin.utilities.jinja import Transcript\n\ntranscript = Transcript(\n    content=\"system: Hello, there! user: Hello, yourself!\",\n    roles=[\"system\", \"user\"],\n)\nprint(transcript.render_to_messages())\n# [\n#   BaseMessage(content='system: Hello, there!', role='system'),\n#   BaseMessage(content='Hello, yourself!', role='user')\n# ]\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.split_text_by_tokens","title":"<code>split_text_by_tokens</code>","text":"<p>Splits a given text by a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be split.</p> required <code>split_tokens</code> <code>list[str]</code> <p>The tokens to split the text by.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>A list of tuples containing the token and the text following it.</p> Example <p>Basic Usage of <code>split_text_by_tokens</code> <pre><code>from marvin.utilities.jinja import split_text_by_tokens\n\ntext = \"Hello, World!\"\nsplit_tokens = [\"Hello\", \"World\"]\npairs = split_text_by_tokens(text, split_tokens)\nprint(pairs) # Output: [(\"Hello\", \", \"), (\"World\", \"!\")]\n</code></pre></p>"},{"location":"api_reference/utilities/logging/","title":"logging","text":""},{"location":"api_reference/utilities/logging/#marvin.utilities.logging","title":"<code>marvin.utilities.logging</code>","text":"<p>Module for logging utilities.</p>"},{"location":"api_reference/utilities/logging/#marvin.utilities.logging.get_logger","title":"<code>get_logger</code>  <code>cached</code>","text":"<p>Retrieves a logger with the given name, or the root logger if no name is given.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the logger to retrieve.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>The logger with the given name, or the root logger if no name is given.</p> Example <p>Basic Usage of <code>get_logger</code> <pre><code>from marvin.utilities.logging import get_logger\n\nlogger = get_logger(\"marvin.test\")\nlogger.info(\"This is a test\") # Output: marvin.test: This is a test\n\ndebug_logger = get_logger(\"marvin.debug\")\ndebug_logger.debug_kv(\"TITLE\", \"log message\", \"green\")\n</code></pre></p>"},{"location":"api_reference/utilities/openai/","title":"openai","text":""},{"location":"api_reference/utilities/openai/#marvin.utilities.openai","title":"<code>marvin.utilities.openai</code>","text":"<p>Module for working with OpenAI.</p>"},{"location":"api_reference/utilities/openai/#marvin.utilities.openai.get_client","title":"<code>get_client</code>","text":"<p>Retrieves an OpenAI client with the given api key and organization.</p> <p>Returns:</p> Type Description <code>AsyncClient</code> <p>The OpenAI client with the given api key and organization.</p> Example <p>Retrieving an OpenAI client <pre><code>from marvin.utilities.openai import get_client\n\nclient = get_client()\n</code></pre></p>"},{"location":"api_reference/utilities/pydantic/","title":"pydantic","text":""},{"location":"api_reference/utilities/pydantic/#marvin.utilities.pydantic","title":"<code>marvin.utilities.pydantic</code>","text":"<p>Module for Pydantic utilities.</p>"},{"location":"api_reference/utilities/pydantic/#marvin.utilities.pydantic.cast_to_model","title":"<code>cast_to_model</code>","text":"<p>Casts a type or callable to a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>function_or_type</code> <code>Union[type, type[BaseModel], GenericAlias, Callable[..., Any]]</code> <p>The type or callable to cast to a Pydantic model.</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the model to create.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>The description of the model to create.</p> <code>None</code> <code>field_name</code> <code>Optional[str]</code> <p>The name of the field to create.</p> <code>None</code> <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>The Pydantic model created from the given type or callable.</p> Example <p>Basic Usage of <code>cast_to_model</code> <pre><code>from marvin.utilities.pydantic import cast_to_model\nfrom pydantic import BaseModel\n\ndef foo(bar: str) -&gt; str:\n    return bar\n\n# cast a function to a model\nmodel = cast_to_model(foo, name=\"Foo\")\nassert issubclass(model, BaseModel)\n</code></pre></p>"},{"location":"api_reference/utilities/pydantic/#marvin.utilities.pydantic.parse_as","title":"<code>parse_as</code>","text":"<p>Parse a given data structure as a Pydantic model via <code>TypeAdapter</code>.</p> <p>Read more about <code>TypeAdapter</code> here.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Any</code> <p>The type to parse the data as.</p> required <code>data</code> <code>Any</code> <p>The data to be parsed.</p> required <code>mode</code> <code>Literal['python', 'json', 'strings']</code> <p>The mode to use for parsing, either <code>python</code>, <code>json</code>, or <code>strings</code>. Defaults to <code>python</code>, where <code>data</code> should be a Python object (e.g. <code>dict</code>).</p> <code>'python'</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The parsed <code>data</code> as the given <code>type_</code>.</p> Example <p>Basic Usage of <code>parse_as</code> <pre><code>from marvin.utilities.pydantic import parse_as\nfrom pydantic import BaseModel\n\nclass ExampleModel(BaseModel):\n    name: str\n\n# parsing python objects\nparsed = parse_as(ExampleModel, {\"name\": \"Marvin\"})\nassert isinstance(parsed, ExampleModel)\nassert parsed.name == \"Marvin\"\n\n# parsing json strings\nparsed = parse_as(\n    list[ExampleModel],\n    '[{\"name\": \"Marvin\"}, {\"name\": \"Arthur\"}]',\n    mode=\"json\"\n)\nassert all(isinstance(item, ExampleModel) for item in parsed)\nassert parsed[0].name == \"Marvin\"\nassert parsed[1].name == \"Arthur\"\n\n# parsing raw strings\nparsed = parse_as(int, '123', mode=\"strings\")\nassert isinstance(parsed, int)\nassert parsed == 123\n</code></pre></p>"},{"location":"api_reference/utilities/slack/","title":"slack","text":""},{"location":"api_reference/utilities/slack/#marvin.utilities.slack","title":"<code>marvin.utilities.slack</code>","text":"<p>Module for Slack-related utilities.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.edit_slack_message","title":"<code>edit_slack_message</code>  <code>async</code>","text":"<p>Edit an existing Slack message by appending new text or replacing it.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <code>str</code> <p>The Slack channel ID.</p> required <code>ts</code> <code>str</code> <p>The timestamp of the message to edit.</p> required <code>new_text</code> <code>str</code> <p>The new text to append or replace in the message.</p> required <code>mode</code> <code>str</code> <p>The mode of text editing, 'append' (default) or 'replace'.</p> <code>'append'</code> <p>Returns:</p> Type Description <code>Response</code> <p>httpx.Response: The response from the Slack API.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.fetch_current_message_text","title":"<code>fetch_current_message_text</code>  <code>async</code>","text":"<p>Fetch the current text of a specific Slack message using its timestamp.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.get_thread_messages","title":"<code>get_thread_messages</code>  <code>async</code>","text":"<p>Get all messages from a slack thread.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.get_token","title":"<code>get_token</code>  <code>async</code>","text":"<p>Get the Slack bot token from the environment.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.search_slack_messages","title":"<code>search_slack_messages</code>  <code>async</code>","text":"<p>Search for messages in Slack workspace based on a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query.</p> required <code>max_messages</code> <code>int</code> <p>The maximum number of messages to retrieve.</p> <code>3</code> <code>channel</code> <code>str</code> <p>The specific channel to search in. Defaults to None, which searches all channels.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of message contents and permalinks matching the query.</p>"},{"location":"api_reference/utilities/strings/","title":"strings","text":""},{"location":"api_reference/utilities/strings/#marvin.utilities.strings","title":"<code>marvin.utilities.strings</code>","text":"<p>Module for string utilities.</p>"},{"location":"api_reference/utilities/tools/","title":"tools","text":""},{"location":"api_reference/utilities/tools/#marvin.utilities.tools","title":"<code>marvin.utilities.tools</code>","text":"<p>Module for NLI tool utilities.</p>"},{"location":"community/","title":"The Marvin Community","text":"<p>We're thrilled you're interested in Marvin! Here, we're all about community. Marvin isn't just a tool, it's a platform for developers to collaborate, learn, and grow. We're driven by a shared passion for making Large Language Models (LLMs) more accessible and easier to use.</p>"},{"location":"community/#connect-on-discord-or-twitter","title":"Connect on Discord or Twitter","text":"<p>The heart of our community beats in our Discord server. It's a space where you can ask questions, share ideas, or just chat with like-minded developers. Don't be shy, join us on Discord or Twitter!</p>"},{"location":"community/#contributing-to-marvin","title":"Contributing to Marvin","text":"<p>Remember, Marvin is your tool. We want you to feel at home suggesting changes, requesting new features, and reporting bugs. Here's how you can contribute:</p> <ul> <li> <p>Issues: Encountered a bug? Have a suggestion? Open an issue in our GitHub repository. We appreciate your input!</p> </li> <li> <p>Pull Requests (PRs): Ready to contribute code? We welcome your pull requests! Not sure how to make a PR? Check out the GitHub guide.</p> </li> <li> <p>Discord Discussions: Have an idea but not quite ready to open an issue or PR? Discuss it with us on Discord first!</p> </li> </ul> <p>Remember, every contribution, no matter how small, is valuable. Don't worry about not being an expert or making mistakes. We're here to learn and grow together. Your input helps Marvin become better for everyone.</p> <p>Stay tuned for community events and more ways to get involved. Marvin is more than a project \u2013 it's a community. And we're excited for you to be a part of it!</p>"},{"location":"community/development_guide/","title":"Development Guide","text":""},{"location":"community/development_guide/#prerequisites","title":"Prerequisites","text":"<p>Marvin requires Python 3.9+.</p>"},{"location":"community/development_guide/#installation","title":"Installation","text":"<p>Clone a fork of the repository and install the dependencies: <pre><code>git clone https://github.com/youFancyUserYou/marvin.git\ncd marvin\n</code></pre></p> <p>Activate a virtual environment: <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> <p>Install the dependencies in editable mode: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> <p>Install the pre-commit hooks: <pre><code>pre-commit install\n</code></pre></p>"},{"location":"community/development_guide/#testing","title":"Testing","text":"<p>Run the tests that don't require an LLM: <pre><code>pytest -vv -m \"not llm\"\n</code></pre></p> <p>Run the LLM tests: <pre><code>pytest -vv -m \"llm\"\n</code></pre></p> <p>Run all tests: <pre><code>pytest -vv\n</code></pre></p>"},{"location":"community/development_guide/#opening-a-pull-request","title":"Opening a Pull Request","text":"<p>Fork the repository and create a new branch: <pre><code>git checkout -b my-branch\n</code></pre></p> <p>Make your changes and commit them: <pre><code>git add . &amp;&amp; git commit -m \"My changes\"\n</code></pre></p> <p>Push your changes to your fork: <pre><code>git push origin my-branch\n</code></pre></p> <p>Open a pull request on GitHub - ping us on Discord if you need help!</p>"},{"location":"community/feedback/","title":"Feedback \ud83d\udc99","text":"<p>We've been humbled and energized by the positive community response to Marvin.</p> <p>Tired: write comments to prompt copilot to write code.Wired: just write comments. it's cleaner :D https://t.co/FOA26lR9xN</p>\u2014 Andrej Karpathy (@karpathy) March 30, 2023 <p>Ok, I admit, I\u2019m getting more and more hyped about @AskMarvinAI. Some of these new functions are pretty legit looking. https://t.co/xhCCKp5kU5</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) April 21, 2023 <p>even the way ai_model uses ai_fn is chefs kiss, truely a craftsmanhttps://t.co/GcmWDeEVJSThey have spinner text.</p>\u2014 jason (@jxnlco) May 12, 2023 <p>The library is open-source: @AskMarvinAI, by @jlowin`@ai_model` is not the only magic Python decorator. There is also `@ai_fn` that makes any function an ambient LLM processor.https://t.co/ZXElyA0Ihp</p>\u2014 Jim Fan (@DrJimFan) May 14, 2023 <p>This is f**king cool. https://t.co/4PH6VAZPYo</p>\u2014 Pydantic (@pydantic) May 14, 2023 <p>Pretty slick\u2026 get Pydantic models from a string of Text. https://t.co/EnnQkzl4Ay</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) May 12, 2023 <p>Uhh how are people not talking about @AskMarvinAI more? @ai_fn is \ud83e\udd2f</p>\u2014 Rushabh Doshi (@radoshi) May 18, 2023"},{"location":"components/ai_classifier/","title":"AI Classifier","text":"<p>AI Classifiers are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p> <code>@ai_classifier</code> is a decorator that lets you use LLMs to choose options, tools, or classify input.    </p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass CustomerIntent(Enum):\n    \"\"\"Classifies the incoming users intent\"\"\"\n\n    SALES = 1\n    TECHNICAL_SUPPORT = 2\n    BILLING_ACCOUNTS = 3\n    PRODUCT_INFORMATION = 4\n    RETURNS_REFUNDS = 5\n    ORDER_STATUS = 6\n    ACCOUNT_CANCELLATION = 7\n    OPERATOR_CUSTOMER_SERVICE = 0\n\n\nCustomerIntent(\"I got double charged, can you help me out?\")\n</code></pre> <pre><code>&lt;CustomerIntent.BILLING_ACCOUNTS: 3&gt;\n</code></pre> <p>How it works</p> <p>     Marvin enumerates your options, and uses a clever logit bias trick to force an LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated with that index.   </p> <p>When to use</p> <p> <ol> <li> Best for classification tasks when no training data is available.      <li> Best for writing classifiers that need deduction or inference.      <p>OpenAI compatibility</p> <p> The technique that AI Classifiers use for speed and correctness is only available through the OpenAI API at this time. Therefore, AI Classifiers can only be used with OpenAI-compatible LLMs, including the Azure OpenAI service.   </p>"},{"location":"components/ai_classifier/#creating-an-ai-classifier","title":"Creating an AI Classifier","text":"<p>AI Classifiers are Python <code>Enums</code>, or classes that can represent one of many possible options. To build an effective AI Classifier, be as specific as possible with your class name, docstring, option names, and option values.</p> <p>To build a minimal AI Classifier, decorate any standard enum, like this:</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass Sentiment(Enum):\n    POSITIVE = \"POSITIVE\"\n    NEGATIVE = \"NEGATIVE\"\n\n\nSentiment(\"That looks great!\")\n</code></pre> <pre><code>&lt;Sentiment.POSITIVE: 'POSITIVE'&gt;\n</code></pre> <p>Because AI Classifiers are enums, you can use any enum construction you want, including the all-caps string approach above, integer values, <code>enum.auto()</code>, or complex values. The only thing to remember is that the class you build is essentially the instruction that gets sent to the LLM, so the more information you provide, the better your classifier will behave.</p> <p>For example, you may want to have a classifier that has a Python object (like an AI Model!) as its value, but still need to provide instruction hints to the LLM. One way to achieve that is to add descriptions to your classifier's values that will become visible to the LLM:</p> <pre><code># dummy objects that stand in for complex tools\nWebSearch = lambda: print(\"Searching!\")\nCalculator = lambda: print(\"Calculating!\")\nTranslator = lambda: print(\"Translating!\")\n\n\n@ai_classifier\nclass Router(Enum):\n    translate = dict(tool=Translator, description=\"A translator tool\")\n    web_search = dict(tool=WebSearch, description=\"A web search tool\")\n    calculator = dict(tool=Calculator, description=\"A calculator tool\")\n\n\nresult = Router(\"Whats 2+2?\")\nresult.value[\"tool\"]()\n</code></pre> <pre><code>Calculating!\n</code></pre>"},{"location":"components/ai_classifier/#configuring-an-ai-classifier","title":"Configuring an AI Classifier","text":"<p>In addition to how you define the AI classifier itself, there are two ways to control its behavior at runtime: <code>instructions</code> and <code>model</code>.</p>"},{"location":"components/ai_classifier/#providing-instructions","title":"Providing instructions","text":"<p>You can control an AI classifier's behavior by providing instructions. This can either be provided globally as the classifier's docstring or on a per-call basis when you instantiate it.</p> <pre><code>@ai_classifier\nclass Sentiment(Enum):\n    \"\"\"\n    Score the sentiment of provided text.\n    \"\"\"\n\n    POSITIVE = 1\n    NEGATIVE = -1\n\n\nSentiment(\"Everything is awesome!\")\n</code></pre> <pre><code>&lt;Sentiment.POSITIVE: 1&gt;\n</code></pre> <pre><code>@ai_classifier\nclass Sentiment(Enum):\n    \"\"\"\n    How would a very very sad person rate the text?\n    \"\"\"\n\n    POSITIVE = 1\n    NEGATIVE = -1\n\n\nSentiment(\"Everything is awesome!\")\n</code></pre> <pre><code>&lt;Sentiment.NEGATIVE: -1&gt;\n</code></pre> <p>Instructions can also be provided for each call:</p> <pre><code>@ai_classifier\nclass Sentiment(Enum):\n    POSITIVE = 1\n    NEGATIVE = -1\n\n\nSentiment(\"Everything is awesome!\", instructions=\"It's opposite day!\")\n</code></pre> <pre><code>&lt;Sentiment.NEGATIVE: -1&gt;\n</code></pre>"},{"location":"components/ai_classifier/#configuring-the-llm","title":"Configuring the LLM","text":"<p>By default, <code>@ai_classifier</code> uses the global LLM settings. To specify a particular LLM, pass it as an argument to the decorator. </p> <pre><code>@ai_classifier(model=\"openai/gpt-3.5-turbo-0613\", temperature = 0)\nclass Sentiment(Enum):\n    POSITIVE = 1\n    NEGATIVE = -1\n\n\nSentiment(\"Everything is awesome!\")\n</code></pre> <pre><code>&lt;Sentiment.POSITIVE: 1&gt;\n</code></pre>"},{"location":"components/ai_classifier/#features","title":"Features","text":""},{"location":"components/ai_classifier/#bulletproof","title":"\ud83d\ude85 Bulletproof","text":"<p><code>ai_classifier</code> will always output one of the options you've given it</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n\nAppRoute(\"update my name\")\n</code></pre> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"components/ai_classifier/#fast","title":"\ud83c\udfc3 Fast","text":"<p><code>ai_classifier</code> only asks your LLM to output one token, so it's blazing fast - on the order of ~200ms in testing.</p>"},{"location":"components/ai_classifier/#deterministic","title":"\ud83e\udee1 Deterministic","text":"<p><code>ai_classifier</code> will be deterministic so long as the underlying model and options does not change.</p>"},{"location":"components/ai_function/","title":"AI Function","text":"<p>AI Functions are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p> <code>@ai_fn</code> is a decorator that lets you use LLMs to generate outputs for Python functions without source code.   </p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef generate_recipe(ingredients: list[str]) -&gt; list[str]:\n    \"\"\"From a list of `ingredients`, generates a\n    complete instruction set to cook a recipe.\n    \"\"\"\n\n\ngenerate_recipe([\"lemon\", \"chicken\", \"olives\", \"coucous\"])\n</code></pre> <p>How it works</p> <p>     AI Functions take your function's name, description, signature, source code, type hints, and provided inputs to predict a likely output. By default, no source code is generated and any existing source code is not executed. The only runtime is the LLM.   </p> <p>When to use</p> <p> <ol> <li> Best for generative tasks: creation and summarization of text or data models.     <li> Best for writing functions that would otherwise be impossible to write.     <li> Great for data extraction, though: see AI Models."},{"location":"components/ai_function/#mapping","title":"Mapping","text":"<p>AI Functions can be mapped over sequences of arguments. Mapped functions run concurrently, which means they run practically in parallel (since they are IO-bound). Therefore, the map will complete as soon as the slowest function call finishes.</p> <p>To see how mapping works, consider this AI Function:</p> <pre><code>@ai_fn\ndef list_fruit(n: int, color: str = None) -&gt; list[str]:\n    \"\"\"\n    Returns a list of `n` fruit that all have the provided `color`\n    \"\"\"\n</code></pre> <p>Mapping is invoked by using the AI Function's <code>.map()</code> method. When mapping, you call the function as you normally would, except that each argument should be a list of items. The function will be called on each set of items (e.g. first with each argument's first item, then with each argument's second item, etc.). For example, this is the same as calling <code>list_fruit(2)</code> and <code>list_fruit(3)</code> concurrently:</p> <pre><code>list_fruit.map([2, 3])\n</code></pre> <pre><code>[['apple', 'banana'], ['apple', 'banana', 'orange']]\n</code></pre> <p>And this is the same as calling <code>list_fruit(2, color='orange')</code> and <code>list_fruit(3, color='red')</code> concurrently:</p> <pre><code>list_fruit.map([2, 3], color=[\"orange\", \"red\"])\n</code></pre> <pre><code>[['orange', 'orange'], ['apple', 'strawberry', 'cherry']]\n</code></pre>"},{"location":"components/ai_function/#features","title":"Features","text":""},{"location":"components/ai_function/#type-safe","title":"\u2699\ufe0f Type Safe","text":"<p><code>ai_fn</code> is fully type-safe. It works out of the box with Pydantic models in your function's parameters or return type.</p> <pre><code>from pydantic import BaseModel\nfrom marvin import ai_fn\n\n\nclass SyntheticCustomer(BaseModel):\n    age: int\n    location: str\n    purchase_history: list[str]\n\n\n@ai_fn\ndef generate_synthetic_customer_data(\n    n: int, locations: list[str], average_purchase_history_length: int\n) -&gt; list[SyntheticCustomer]:\n    \"\"\"Generates synthetic customer data based on the given parameters.\n    Parameters include the number of customers ('n'),\n    a list of potential locations, and the average length of a purchase history.\n    \"\"\"\n\n\ncustomers = generate_synthetic_customer_data(\n    5, [\"New York\", \"San Francisco\", \"Chicago\"], 3\n)\n</code></pre>"},{"location":"components/ai_function/#natural-language-api","title":"\ud83d\udde3\ufe0f Natural Language API","text":"<p>Marvin exposes an API to prompt an <code>ai_fn</code> with natural language. This lets you create a Language API for any function you can write down.</p> <pre><code>generate_synthetic_customer_data.prompt(\n    \"I need 10 profiles from rural US cities making between 3 and 7 purchases\"\n)\n</code></pre> <p>\ud83e\uddea Code Generation</p> <p>By default, no code is generated or executed when you call an <code>ai_fn</code>. For those who wish to author code, Marvin exposes an experimental API for code generation. Simply call <code>.code()</code> on an ai_fn, and Marvin will generate the code for you. By default, Marvin will write python code. You can pass a language keyword to generate code in other languages, i.e. <code>.code(language = 'rust')</code>. For best performance give your function a good name, with descriptive docstring, and a signature with type-hints. Provided code will be interpreted as pseudocode. </p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef fibonacci(n: int) -&gt; int:\n    \"\"\"\n    Returns the nth number in the Fibonacci sequence.\n    \"\"\"\n\n\nfibonacci.code(language=\"rust\")\n</code></pre>"},{"location":"components/ai_function/#examples","title":"Examples","text":""},{"location":"components/ai_function/#customer-sentiment","title":"Customer Sentiment","text":"<p>Rapidly prototype natural language pipelines.</p> <p>     Use hallucination as a literal feature. Generate data that would be impossible     or prohibatively expensive to purchase as you rapidly protype NLP pipelines.    </p> <pre><code>@ai_fn\ndef analyze_customer_sentiment(reviews: list[str]) -&gt; dict:\n    \"\"\"\n    Returns an analysis of customer sentiment, including common\n    complaints, praises, and suggestions, from a list of product\n    reviews.\n    \"\"\"\n\n\n# analyze_customer_sentiment([\"I love this product!\", \"I hate this product!\"])\n</code></pre>"},{"location":"components/ai_function/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<p>General real fake data.</p> <p>     Use hallucination as a figurative feature. Use python or pydantic     to describe the data model you need, and generate realistic data on the fly      for sales demos.   </p> <pre><code>class FinancialReport(pydantic.BaseModel):\n    ...\n\n\n@ai_fn\ndef create_drip_email(n: int, market_conditions: str) -&gt; list[FinancialReport]:\n    \"\"\"\n    Generates `n` synthetic financial reports based on specified\n    `market_conditions` (e.g., 'recession', 'bull market', 'stagnant economy').\n    \"\"\"\n</code></pre> <pre><code>class IoTData(pydantic.BaseModel):\n    ...\n\n\n@ai_fn\ndef generate_synthetic_IoT_data(n: int, device_type: str) -&gt; list[IoTData]:\n    \"\"\"\n    Generates `n` synthetic data points mimicking those from a specified\n    `device_type` in an IoT system.\n    \"\"\"\n</code></pre>"},{"location":"components/ai_model/","title":"AI Model","text":"<p>AI Models are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p>A decorator that lets you extract structured data from unstructured text, documents, or instructions.</p> <p>Example</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str = Field(..., description=\"The two-letter state abbreviation\")\n\n# We can now put pass unstructured context to this model.\nLocation(\"The Big Apple\")\n</code></pre> Returns <pre><code>Location(city='New York', state='NY')\n</code></pre> <p>How it works</p> <p>AI Models use an LLM to extract, infer, or deduce data from the provided text. The data is parsed with Pydantic into the provided schema.</p> <p>When to use</p> <ul> <li>Best for extractive tasks: structuing of text or data models.</li> <li>Best for writing NLP pipelines that would otherwise be impossible to create.</li> <li>Good for model generation, though, see AI Function.</li> </ul>"},{"location":"components/ai_model/#creating-an-ai-model","title":"Creating an AI Model","text":"<p>AI Models are identical to Pydantic <code>BaseModels</code>, except that they can attempt to parse natural language to populate their fields. To build an effective AI Model, be as specific as possible with your field names, field descriptions, docstring, and instructions.</p> <p>To build a minimal AI model, decorate any standard Pydantic model, like this:</p> <p>Example</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    \"\"\"A representation of a US city and state\"\"\"\n\n    city: str = Field(description=\"The city's proper name\")\n    state: str = Field(description=\"The state's two-letter abbreviation (e.g. NY)\")\n\n# We can now put pass unstructured context to this model.\nLocation(\"The Big Apple\")\n</code></pre> Returns <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"components/ai_model/#configuring-an-ai-model","title":"Configuring an AI Model","text":"<p>In addition to how you define the AI model itself, there are two ways to control its behavior at runtime: <code>instructions</code> and <code>model</code>.</p>"},{"location":"components/ai_model/#providing-instructions","title":"Providing instructions","text":"<p>When parsing text, AI Models can take up to three different forms of instruction: - the AI Model's docstring (set at the class level) - instructions passed to the <code>@ai_model</code> decorator (set at the class level) - instructions passed to the AI Model when instantiated (set at the instance / call level)</p> <p>The AI Model's docstring and the <code>@ai_model</code> instructions are roughly equivalent: they are both provided when the class is defined, not when it is instantiated, and are therefore applied to every instance of the class. Users can choose to put information in either location. If you only want to use one, our recommendation is to use the docstring for clarity. Alternatively, you may prefer to put the model's documentation in the docstring (as you would for a normal Pydantic model) and put parsing instructions in the <code>@ai_model</code> decorator, since those are unique to the LLM. This is entirely a matter of preference and users should opt for whichever is more clear; both the docstring and the <code>@ai_model</code> instructions are provided to the LLM in the same way.</p> <p>Here is an example of an AI model with a documentation docstring and parsing instructions provided to the decorator:</p> <pre><code>@ai_model(instructions=\"Translate to French\")\nclass Translation(BaseModel):\n    \"\"\"A record of original text and translated text\"\"\"\n\n    original_text: str\n    translated_text: str\n\n\nTranslation(\"Hello, world!\")\n</code></pre> <pre><code>Translation(original_text='Hello, world!', translated_text='Bonjour le monde!')\n</code></pre> <p>In the above case, we could have also put \"translate to French\" in the docstring (and perhaps renamed the object <code>FrenchTranslation</code>, since that's the only language it can represent).</p> <p>The third opportunity to provide instructions is when the model is actually instantiated. These instructions are combined with any other instructions to guide the model behavior. Here's how we could use the same <code>Translation</code> object to handle multiple languages:</p> <pre><code>@ai_model\nclass Translation(BaseModel):\n    \"\"\"A record of original text and translated text\"\"\"\n\n    original_text: str\n    translated_text: str\n\n\nprint(Translation(\"Hello, world!\", instructions_=\"Translate to French\"))\nprint(Translation(\"Hello, world!\", instructions_=\"Translate to German\"))\n</code></pre> <pre><code>original_text='Hello, world!' translated_text='Bonjour, le monde!'\noriginal_text='Hello, world!' translated_text='Hallo, Welt!'\n</code></pre> <p>Note that the kwarg is <code>instructions_</code> with a trailing underscore; this is to avoid conflicts with models that may have a real <code>instructions</code> field. If you accidentally pass \"instructions\" to a model without an \"instructions\" field, a helpful error will identify your mistake.</p> <p>Putting this all together, here is a model whose behavior is informed by a docstring on the class itself, an instruction provided to the decorator, and an instruction provided to the instance.</p> <pre><code>@ai_model(instructions=\"Always set color_2 to 'red'\")\nclass Test(BaseModel):\n    \"\"\"Always set color_1 to 'orange'\"\"\"\n\n    color_1: str\n    color_2: str\n    color_3: str\n\n\nt1 = Test(\"Hello\", instructions_=\"Always set color_3 to 'blue'\")\nassert t1 == Test(color_1=\"orange\", color_2=\"red\", color_3=\"blue\")\n</code></pre>"},{"location":"components/ai_model/#configuring-the-llm","title":"Configuring the LLM","text":"<p>By default, <code>@ai_model</code> uses the global LLM settings. To specify a particular LLM, pass it as an argument to the decorator or at instantiation. If you provide it to the decorator, it becomes the default for all uses of that model. If you provide it at instantiation, it is only used for that specific model. </p> <p>Note that the kwarg is <code>model_</code> with a trailing underscore; this is to avoid conflicts with models that may have a real <code>model</code> field. If you accidentally pass a \"model\" kwarg and there is no \"model\" field, a helpful error will identify your mistake.</p> <pre><code>@ai_model(model=\"openai/gpt-3.5-turbo\", temperature=0)\nclass Location(BaseModel):\n    city: str\n    state: str\n\n\nprint(Location(\"The Big Apple\"))\n</code></pre> <pre><code>city='New York' state='New York'\n</code></pre>"},{"location":"components/ai_model/#features","title":"Features","text":""},{"location":"components/ai_model/#type-safe","title":"\u2699\ufe0f Type Safe","text":"<p><code>ai_model</code> is fully type-safe. It works out of the box with Pydantic models.</p>"},{"location":"components/ai_model/#powered-by-deduction","title":"\ud83e\udde0 Powered by deduction","text":"<p><code>ai_model</code> gives your data model access to the knowledge and deductive power  of a Large Language Model. This means that your data model can infer answers to previous impossible tasks.</p> <pre><code>@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str\n    country: str\n    latitude: float\n    longitude: float\n\n\nLocation(\"He says he's from the windy city\")\n\n# Location(\n#   city='Chicago',\n#   state='Illinois',\n#   country='United States',\n#   latitude=41.8781,\n#   longitude=-87.6298\n# )\n</code></pre>"},{"location":"components/ai_model/#examples","title":"Examples","text":""},{"location":"components/ai_model/#resumes","title":"Resumes","text":"<pre><code>from typing import Optional\nfrom pydantic import BaseModel\nfrom marvin import ai_model\n\n\n@ai_model\nclass Resume(BaseModel):\n    first_name: str\n    last_name: str\n    phone_number: Optional[str]\n    email: str\n\n\nResume(\"Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io\").json(indent=2)\n\n# {\n# first_name: 'Ford',\n# last_name: 'Prefect',\n# email: 'ford@prefect.io',\n# phone: '(555) 5124-5242',\n# }\n</code></pre>"},{"location":"components/ai_model/#customer-service","title":"Customer Service","text":"<pre><code>import datetime\nfrom typing import Optional, List\nfrom pydantic import BaseModel\nfrom marvin import ai_model\n\n\nclass Destination(pydantic.BaseModel):\n    start: datetime.date\n    end: datetime.date\n    city: Optional[str]\n    country: str\n    suggested_attractions: list[str]\n\n\n@ai_model\nclass Trip(pydantic.BaseModel):\n    trip_start: datetime.date\n    trip_end: datetime.date\n    trip_preferences: list[str]\n    destinations: List[Destination]\n\n\nTrip(\"\"\"\\\n    I've got all of June off, so hoping to spend the first\\\n    half of June in London and the second half in Rabat. I love \\\n    good food and going to museums.\n\"\"\").json(indent=2)\n\n# {\n#   \"trip_start\": \"2023-06-01\",\n#   \"trip_end\": \"2023-06-30\",\n#   \"trip_preferences\": [\n#     \"good food\",\n#     \"museums\"\n#   ],\n#   \"destinations\": [\n#     {\n#       \"start\": \"2023-06-01\",\n#       \"end\": \"2023-06-15\",\n#       \"city\": \"London\",\n#       \"country\": \"United Kingdom\",\n#       \"suggested_attractions\": [\n#         \"British Museum\",\n#         \"Tower of London\",\n#         \"Borough Market\"\n#       ]\n#     },\n#     {\n#       \"start\": \"2023-06-16\",\n#       \"end\": \"2023-06-30\",\n#       \"city\": \"Rabat\",\n#       \"country\": \"Morocco\",\n#       \"suggested_attractions\": [\n#         \"Kasbah des Oudaias\",\n#         \"Hassan Tower\",\n#         \"Rabat Archaeological Museum\"\n#       ]\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/ai_model/#electronic-health-records","title":"Electronic Health Records","text":"<pre><code>from datetime import date\nfrom typing import Optional, List\nfrom pydantic import BaseModel\n\n\nclass Patient(BaseModel):\n    name: str\n    age: int\n    is_smoker: bool\n\n\nclass Diagnosis(BaseModel):\n    condition: str\n    diagnosis_date: date\n    stage: Optional[str] = None\n    type: Optional[str] = None\n    histology: Optional[str] = None\n    complications: Optional[str] = None\n\n\nclass Treatment(BaseModel):\n    name: str\n    start_date: date\n    end_date: Optional[date] = None\n\n\nclass Medication(Treatment):\n    dose: Optional[str] = None\n\n\nclass BloodTest(BaseModel):\n    name: str\n    result: str\n    test_date: date\n\n\n@ai_model\nclass PatientData(BaseModel):\n    patient: Patient\n    diagnoses: List[Diagnosis]\n    treatments: List[Treatment]\n    blood_tests: List[BloodTest]\n\n\nPatientData(\"\"\"\\\nMs. Lee, a 45-year-old patient, was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nUnfortunately, Ms. Lee's diabetes has progressed and she developed diabetic retinopathy on 09-01-2019.\nMs. Lee was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nMs. Lee was initially diagnosed with stage I hypertension on 06-01-2018.\nMs. Lee's blood work revealed hyperlipidemia with elevated LDL levels on 06-01-2018.\nMs. Lee was prescribed metformin 1000 mg daily for her diabetes on 06-01-2018.\nMs. Lee's most recent A1C level was 8.5% on 06-15-2020.\nMs. Lee was diagnosed with type 2 diabetes mellitus, with microvascular complications, including diabetic retinopathy, on 09-01-2019.\nMs. Lee's blood pressure remains elevated and she was prescribed lisinopril 10 mg daily on 09-01-2019.\nMs. Lee's most recent lipid panel showed elevated LDL levels, and she was prescribed atorvastatin 40 mg daily on 09-01-2019.\\\n\"\"\").json(indent=2)\n\n# {\n#   \"patient\": {\n#     \"name\": \"Ms. Lee\",\n#     \"age\": 45,\n#     \"is_smoker\": false\n#   },\n#   \"diagnoses\": [\n#     {\n#       \"condition\": \"Type 2 diabetes mellitus\",\n#       \"diagnosis_date\": \"2018-06-01\",\n#       \"stage\": \"I\",\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     },\n#     {\n#       \"condition\": \"Diabetic retinopathy\",\n#       \"diagnosis_date\": \"2019-09-01\",\n#       \"stage\": null,\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     }\n#   ],\n#   \"treatments\": [\n#     {\n#       \"name\": \"Metformin\",\n#       \"start_date\": \"2018-06-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Lisinopril\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Atorvastatin\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     }\n#   ],\n#   \"blood_tests\": [\n#     {\n#       \"name\": \"A1C\",\n#       \"result\": \"8.5%\",\n#       \"test_date\": \"2020-06-15\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2018-06-01\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2019-09-01\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/ai_model/#text-to-sql","title":"Text to SQL","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom django.db.models import Q\n\n\nclass DjangoLookup(BaseModel):\n    field: Literal[*django_fields]\n    lookup: Literal[*django_lookups] = pydantic.Field(description=\"e.g. __iregex\")\n    value: Any\n\n\n@ai_model\nclass DjangoQuery(BaseModel):\n    \"\"\"A model representing a Django ORM query\"\"\"\n\n    lookups: List[DjangoLookup]\n\n    def to_q(self) -&gt; Q:\n        q = Q()\n        for lookup in self.lookups:\n            q &amp;= Q(**{f\"{lookup.field}__{lookup.lookup}\": lookup.value})\n        return q\n\n\nDjangoQuery(\"\"\"\\\n    All users who joined more than two months ago but\\\n    haven't made a purchase in the last 30 days\"\"\").to_q()\n\n# &lt;Q: (AND:\n#     ('date_joined__lte', '2023-03-11'),\n#     ('last_purchase_date__isnull', False),\n#     ('last_purchase_date__lte', '2023-04-11'))&gt;\n</code></pre>"},{"location":"components/ai_model/#financial-reports","title":"Financial Reports","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\n\n\n@ai_model\nclass CapTable(BaseModel):\n    total_authorized_shares: int\n    total_common_share: int\n    total_common_shares_outstanding: Optional[int]\n    total_preferred_shares: int\n    conversion_price_multiple: int = 1\n\n\nCapTable(\"\"\"\\\n    In the cap table for Charter, the total authorized shares amount to 13,250,000. \n    The total number of common shares stands at 10,000,000 as specified in Article Fourth, \n    clause (i) and Section 2.2(a)(i). The exact count of common shares outstanding is not \n    available at the moment. Furthermore, there are a total of 3,250,000 preferred shares mentioned \n    in Article Fourth, clause (ii) and Section 2.2(a)(ii). The dividend percentage for Charter is \n    set at 8.00%. Additionally, the mandatory conversion price multiple is 3x, which is \n    derived from the Term Sheet.\\\n\"\"\").json(indent=2)\n\n# {\n#   \"total_authorized_shares\": 13250000,\n#   \"total_common_share\": 10000000,\n#   \"total_common_shares_outstanding\": null,\n#   \"total_preferred_shares\": 3250000,\n#   \"conversion_price_multiple\": 3\n# }\n</code></pre>"},{"location":"components/ai_model/#meeting-notes","title":"Meeting Notes","text":"<pre><code>import datetime\nfrom typing import List\nfrom pydantic import BaseModel\nfrom typing_extensions import Literal\nfrom marvin import ai_model\n\n\nclass ActionItem(BaseModel):\n    responsible: str\n    description: str\n    deadline: Optional[datetime.datetime]\n    time_sensitivity: Literal[\"low\", \"medium\", \"high\"]\n\n\n@ai_model\nclass Conversation(BaseModel):\n    \"\"\"A class representing a team conversation\"\"\"\n\n    participants: List[str]\n    action_items: List[ActionItem]\n\n\nConversation(\"\"\"\n    Adam: Hey Jeremiah can you approve my PR? I requested you to review it.\n    Jeremiah: Yeah sure, when do you need it done by?\n    Adam: By this Friday at the latest, we need to ship it by end of week.\n    Jeremiah: Oh shoot, I need to make sure that Nate and I have a chance to chat first.\n    Nate: Jeremiah we can meet today to chat.\n    Jeremiah: Okay, I'll book something for today.\n\"\"\").json(indent=2)\n\n# {\n#   \"participants\": [\n#     \"Adam\",\n#     \"Jeremiah\",\n#     \"Nate\"\n#   ],\n#   \"action_items\": [\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Approve Adam's PR\",\n#       \"deadline\": \"2023-05-12T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     },\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Book a meeting with Nate\",\n#       \"deadline\": \"2023-05-11T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/overview/","title":"AI Components","text":"<p>Marvin introduces a number of components that can become the building blocks of AI-powered software.</p> <p>Selecting a backing LLM</p> <p>When using any of Marvin's components, the LLM used will default to the value of <code>MARVIN_LLM_MODEL</code>. To override this on a per-call basis, pass the <code>model</code> argument to the component's decorator.</p> <p>For example, to use <code>openai/gpt-3.5-turbo-16k</code> for an <code>ai_fn</code> call, you would do the following:</p> <pre><code>@ai_fn(model=\"openai/gpt-3.5-turbo-16k\", temperature = 0)\ndef my_ai_fn():\n    \"\"\"...\"\"\"\n</code></pre>"},{"location":"components/overview/#ai-models","title":"AI Models","text":"<p>Marvin's most basic component is the AI Model, a drop-in replacement for Pydantic's <code>BaseModel</code>. AI Models can be instantiated from any string, making them ideal for structuring data, entity extraction, and synthetic data generation:</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str = Field(..., description=\"The two-letter state abbreviation\")\n\n\nLocation(\"The Big Apple\")\n</code></pre> <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"components/overview/#ai-classifiers","title":"AI Classifiers","text":"<p>AI Classifiers let you build multi-label classifiers with no code and no training data. It enumerates your options, and uses a clever logit bias trick to force an LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated to that index. It's bulletproof, cost-effective, and lets you build classifiers as quickly as you can write your classes.</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n\nAppRoute(\"update my name\")\n</code></pre> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"components/overview/#ai-functions","title":"AI Functions","text":"<p>AI Functions look like regular functions, but have no source code. Instead, an AI uses their description and inputs to generate their outputs, making them ideal for NLP applications like sentiment analysis. </p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment(text: str) -&gt; float:\n    \"\"\"\n    Given `text`, returns a number between 1 (positive) and -1 (negative)\n    indicating its sentiment score.\n    \"\"\"\n\n\nprint(\"Text 1:\", sentiment(\"I love working with Marvin!\"))\nprint(\"Text 2:\", sentiment(\"These examples could use some work...\"))\n</code></pre> <pre><code>Text 1: 0.8\nText 2: -0.2\n</code></pre> <p>Because AI functions are just like regular functions, you can quickly modify them for your needs. Here, we modify the above example to work with multiple strings at once:</p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n    \"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\n\n\nsentiment_list(\n    [\n        \"That was surprisingly easy!\",\n        \"Oh no, not again.\",\n    ]\n)\n</code></pre> <pre><code>[0.7, -0.5]\n</code></pre>"},{"location":"components/overview/#ai-applications","title":"AI Applications","text":"<p>AI Applications are the base class for interactive use cases. They are designed to be invoked one or more times, and automatically maintain three forms of state:</p> <ul> <li><code>state</code>: a structured application state</li> <li><code>plan</code>: high-level planning for the AI assistant to keep the application \"on-track\" across multiple invocations</li> <li><code>history</code>: a history of all LLM interactions</li> </ul> <p>AI Applications can be used to implement many \"classic\" LLM use cases, such as chatbots, tool-using agents, developer assistants, and more. In addition, thanks to their persistent state and planning, they can implement applications that don't have a traditional chat UX, such as a ToDo app. Here's an example:</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom marvin import AIApplication\n\n\n# create models to represent the state of our ToDo app\nclass ToDo(BaseModel):\n    title: str\n    description: str = None\n    due_date: datetime = None\n    done: bool = False\n\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n\n\n# create the app with an initial state and description\ntodo_app = AIApplication(\n    state=ToDoState(),\n    description=(\n        \"A simple todo app. Users will provide instructions for creating and updating\"\n        \" their todo lists.\"\n    ),\n)\n</code></pre> <p>Now we can invoke the app directly to add a to-do item. Note that the app understands that it is supposed to manipulate state, not just respond conversationally.</p> <pre><code># invoke the application by adding a todo\nresponse = todo_app(\"I need to go to the store tomorrow at 5pm\")\n\n\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Got it! I've added a new task to your to-do list. You need to go to the store tomorrow at 5pm.\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": \"Buy groceries\",\n      \"due_date\": \"2023-07-12T17:00:00+00:00\",\n      \"done\": false\n    }\n  ]\n}\n</code></pre> <p>We can inform the app that we already finished the task, and it updates state appropriately</p> <pre><code># complete the task\nresponse = todo_app(\"I already went\")\n\n\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Great! I've marked the task as completed. Is there anything else you'd like to add to your to-do list?\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": \"Buy groceries\",\n      \"due_date\": \"2023-07-12T17:00:00+00:00\",\n      \"done\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"configuration/settings/","title":"Settings","text":"<p>Marvin makes use of Pydantic's <code>BaseSettings</code> for configuration throughout the package.</p>"},{"location":"configuration/settings/#environment-variables","title":"Environment Variables","text":"<p>All settings are configurable via environment variables like <code>MARVIN_&lt;setting name&gt;</code>.</p> <p>For example, in an <code>.env</code> file or in your shell config file you might have: <pre><code>MARVIN_LOG_LEVEL=DEBUG\nMARVIN_LLM_MODEL=openai/gpt-4\nMARVIN_LLM_TEMPERATURE=0\n</code></pre></p>"},{"location":"configuration/settings/#runtime-settings","title":"Runtime Settings","text":"<p>A runtime settings object is accessible via <code>marvin.settings</code> and can be used to access or update settings throughout the package.</p> <p>For example, to access or change the LLM model used by Marvin at runtime: <pre><code>import marvin\n\nmarvin.settings.llm_model\n# 'openai/gpt-4'\n\nmarvin.settings.llm_model = 'openai/gpt-3.5-turbo'\n\nmarvin.settings.llm_model\n# 'openai/gpt-3.5-turbo'\n</code></pre></p>"},{"location":"configuration/settings/#llm-providers","title":"LLM Providers","text":"<p>Marvin supports multiple LLM providers, including OpenAI and Anthropic. After configuring your credentials appropriately, you can use any supported model by setting <code>marvin.settings.llm_model</code> appropriately. </p> <p>Valid <code>llm_model</code> settings are strings with the form <code>\"{provider_key}/{model_name}\"</code>. For example, <code>\"openai/gpt-3.5-turbo\"</code>, <code>anthropic/claude-2</code>, or <code>azure_openai/gpt-4</code>.</p> Provider Provider Key Models Notes OpenAI <code>openai</code> <code>gpt-3.5-turbo</code>, <code>gpt-4</code> (default), or any other compatible model Marvin is generally tested and optimized with this provider. Anthropic <code>anthropic</code> <code>claude-2</code>, <code>claude-instant-1</code> or any other compatible model Available in Marvin 1.1 Azure OpenAI Service <code>azure_openai</code> <code>gpt-3.5-turbo</code>, <code>gpt-4</code>, or any other compatible model The Azure OpenAI Service shares all the same configuration options as the OpenAI models, as well as a few additional ones. Available in Marvin 1.1."},{"location":"configuration/settings/#llm-configuration","title":"LLM Configuration","text":"<p>To configure LLM models globally, you can adjust the following settings. Note that these become the default settings for all models, but you can always set these on a per-model or per-component basis.</p> Setting Env Variable Runtime Variable Default Notes LLM model <code>MARVIN_LLM_MODEL</code> <code>marvin.settings.llm_model</code> <code>openai/gpt-3.5-turbo</code> Set the model as <code>{provider}/{model}</code>. Defaults to OpenAI's GPT-3.5 model. Temperature <code>MARVIN_LLM_TEMPERATURE</code> <code>marvin.settings.llm_temperature</code> 0.8 Max tokens <code>MARVIN_LLM_MAX_TOKENS</code> <code>marvin.settings.llm_max_tokens</code> 1500 The maximum number of tokens in a model completion Timeout <code>MARVIN_LLM_REQUEST_TIMEOUT_SECONDS</code> <code>marvin.settings.llm_request_timeout_seconds</code> 600.0"},{"location":"examples/classification_api/","title":"Basic Classifier API","text":"<p>With Marvin, you can easily build a production-grade application or data pipeline to classify data from unstructured text.  In this example, we'll show how to </p> <ul> <li>Write an AI-powered Function.</li> <li>Build a production-ready API.</li> </ul>"},{"location":"examples/classification_api/#writing-an-ai-powered-function","title":"Writing an AI-powered Function.","text":"<p>Example</p> <p>Marvin translates your Python code into English, passes that to an Large Language Model,  and parses its response. It uses AI to evaluate your function, no code required.</p> <p>Let's build a function that classifies a paragraph of text into predefined categories.  This is typically a pretty daunting task for a machine learning savant, much less your average engineer.  Marvin lets you accomplish this by writing code the way you normally would, no PhD required.</p> <p>We'll simply write a Python function, tell it that we expect a <code>text</code> input and that it'll  output a <code>str</code>, or string. With Marvin, we'll use <code>ai_fn</code> and decorate this function.  When we do, this function will use AI to get its answer.</p> <p><pre><code>from marvin import ai_fn, settings\nfrom typing import Literal\n\nsettings.openai.api_key = 'API_KEY' \n\n@ai_fn\ndef classify_text(text: str) -&gt; Literal['sports', 'politics', 'technology']:\n    '''\n        Correctly classifies the passed `text` into one of the predefined categories. \n    '''\n</code></pre> This function can now be run! When we test it out, we get great results.</p> Results <pre><code>classify_text('The Lakers won the game last night')\n\n# returns Category.SPORTS\n</code></pre>"},{"location":"examples/classification_api/#build-a-production-ready-api","title":"Build a production-ready API.","text":"<p>In the following example, we will demonstrate how to deploy the AI function we just created as an API using FastAPI. FastAPI is a powerful tool that allows us to easily turn our AI function into a fully-fledged API. This API can then be used by anyone to send a POST request to our <code>/classify_text/</code> endpoint and get the classified category they need. Let's see how this can be done.</p> <p>Example</p> <p>Now that we have our AI function, let's deploy it as an API using FastAPI. FastAPI is a modern, fast (high-performance), web framework for building APIs.</p> <pre><code>from fastapi import FastAPI\nfrom marvin import ai_fn, settings\nfrom typing import Literal\n\nsettings.openai.api_key = 'API_KEY' \n\napp = FastAPI()\n\nsettings.openai.api_key = 'API_KEY'\n\n@app.post(\"/classify_text/\")\n@ai_fn\ndef classify_text(text: str) -&gt; Literal['sports', 'politics', 'technology']:\n    '''\n        Correctly classifies the passed `text` into one of the predefined categories. \n    '''\n</code></pre> <p>With just a few lines of code, we've turned our AI function into a fully-fledged API. Now, anyone can send a POST request to our <code>/classify_text/</code> endpoint and get the classified category they need.</p> API Deployment <p><pre><code>import uvicorn\nimport asyncio\n\nconfig = uvicorn.Config(app)\nserver = uvicorn.Server(config)\nasyncio.run(server.serve())\n</code></pre> Now, you can navigate to localhost:8000/docs to interact with your API.</p> Making Requests <p><pre><code>import requests\n\ndata = {\"text\": \"The Lakers won the game last night\"}\nresponse = requests.post(\"http://localhost:8000/classify_text/\", json=data)\nprint(response.json())\n\n# returns 'SPORTS'\n</code></pre> This will send a POST request to the <code>/classify_text/</code> endpoint with the provided text and print the response.</p>"},{"location":"examples/deduplication/","title":"Entity Deduplication","text":"<p>What is entity deduplication?</p> <p>How many distinct cities are mentioned in the following text:</p> <p>Chicago, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco. </p> <p>We know it's three, but getting software to deduplicate these entities is surprisingly hard. </p> <p>How can we turn it into something cleaner like:</p> <pre><code>[\n    City(text='Chicago', inferred_city='Chicago'),\n    City(text='The Windy City', inferred_city='Chicago'),\n    City(text='New York City', inferred_city='New York City'),\n    City(text='The Big Apple', inferred_city='New York City'),\n    City(text='SF', inferred_city='San Francisco'),\n    City(text='San Fran', inferred_city='San Francisco'),\n    City(text='San Francisco', inferred_city='San Francisco')\n]\n</code></pre> <p>In this example, we'll explore how you can do text and entity deduplication from a piece of text. </p>"},{"location":"examples/deduplication/#creating-our-data-model","title":"Creating our data model","text":"<p>To extract and deduplicate entities, we'll want to think carefully about the data we want to extract from this text. We clearly want a <code>list</code> of <code>cities</code>. So we'll want to create a data model to represent a city. But we won't stop there:  we don't want to just get a list of cities that appear in the text. We want to get an mapping or understanding that SF is the same as San Francisco, and the Big Apple is the same as New York City, etc. </p> <pre><code>import pydantic\n\nclass City(pydantic.BaseModel):\n    '''\n        A model to represent a city.\n    '''\n\n    text: str = pydantic.Field(\n        description = 'The city name as it appears'\n    )\n\n    inferred_city: str = pydantic.Field(\n            description = 'The inferred and normalized city name.'\n        )\n</code></pre>"},{"location":"examples/deduplication/#creating-our-prompt","title":"Creating our prompt","text":"<p>Now we'll need to use this model and convert it into a prompt we can send to a language model. We'll use Marvin's prompt_fn to let us write a prompt like a python function. </p> <pre><code>from marvin import prompt_fn\n\n@prompt_fn\ndef get_cities(text: str) -&gt; list[City]:\n    '''\n        Expertly deduce and infer all cities from the follwing text: {{ text }}\n    '''\n</code></pre> What does get_cities do under the hood? <p>Marvin's <code>prompt_fn</code> only creates a prompt to send to a large language model. It does not call any  external service, it's simply responsible for translating your query into something that a  large language model will understand. </p> <p>Here's the output when we plug in our sentence from above:</p> <pre><code>get_cities(\"Chicago, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco.\")\n</code></pre> Click to see output <pre><code>{\n\"messages\": [\n    {\n    \"role\": \"system\",\n    \"content\": \"Expertly deduce and infer all cities from the follwing text: Chicago, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco.\"\n    }\n],\n\"functions\": [\n    {\n    \"parameters\": {\n        \"$defs\": {\n        \"City\": {\n            \"description\": \"A model to represent a city.\",\n            \"properties\": {\n            \"text\": {\n                \"description\": \"The city name as it appears\",\n                \"title\": \"Text\",\n                \"type\": \"string\"\n            },\n            \"inferred_city\": {\n                \"description\": \"The inferred and normalized city name.\",\n                \"title\": \"Inferred City\",\n                \"type\": \"string\"\n            }\n            },\n            \"required\": [\n            \"text\",\n            \"inferred_city\"\n            ],\n            \"title\": \"City\",\n            \"type\": \"object\"\n        }\n        },\n        \"properties\": {\n        \"output\": {\n            \"items\": {\n            \"$ref\": \"#/$defs/City\"\n            },\n            \"title\": \"Output\",\n            \"type\": \"array\"\n        }\n        },\n        \"required\": [\n        \"output\"\n        ],\n        \"type\": \"object\"\n    },\n    \"name\": \"Output\",\n    \"description\": \"\"\n    }\n],\n\"function_call\": {\n    \"name\": \"Output\"\n}\n}\n</code></pre>"},{"location":"examples/deduplication/#calling-our-language-model","title":"Calling our Language Model","text":"<p>Let's see what happens when we actually call our Large Language Model. Below, <code>**</code> tells let's us pass the prompt's parameters into our call to OpenAI.</p> <pre><code>import openai\nimport json\n\nresponse = openai.ChatCompletion.create(\n    api_key = 'YOUR OPENAI KEY',\n    model = 'gpt-3.5-turbo',\n    temperature = 0,\n    **get_cities(\n        (\n            \"Chicago, The Windy City, New York City, \"\n            \"The Big Apple, SF, San Fran, San Francisco.\"\n        )\n    )\n)\n</code></pre> View the raw response <p>The raw response we receive looks like  <pre><code>{\n    \"id\": \"omitted for this example\",\n    \"object\": \"chat.completion\",\n    \"created\": 1697222527,\n    \"model\": \"gpt-3.5-turbo-0613\",\n    \"choices\": [\n        {\n        \"index\": 0,\n        \"message\": {\n            \"role\": \"assistant\",\n            \"content\": null,\n            \"function_call\": {\n            \"name\": \"Output\",\n            \"arguments\": \"{\\n  \\\"output\\\": [\\n    {\\n      \\\"text\\\": \\\"Chicago\\\",\\n      \\\"inferred_city\\\": \\\"Chicago\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"The Windy City\\\",\\n      \\\"inferred_city\\\": \\\"Chicago\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"New York City\\\",\\n      \\\"inferred_city\\\": \\\"New York City\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"The Big Apple\\\",\\n      \\\"inferred_city\\\": \\\"New York City\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"SF\\\",\\n      \\\"inferred_city\\\": \\\"San Francisco\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"San Fran\\\",\\n      \\\"inferred_city\\\": \\\"San Francisco\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"San Francisco\\\",\\n      \\\"inferred_city\\\": \\\"San Francisco\\\"\\n    }\\n  ]\\n}\"\n            }\n        },\n        \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 87,\n        \"completion_tokens\": 165,\n        \"total_tokens\": 252\n    }\n}\n</code></pre></p> <p>We can parse the raw response and mine out the relevant responses, </p> <pre><code>[\n    City.parse_obj(city) \n    for city in \n    json.loads(\n        response.choices[0].message.function_call.arguments\n    ).get('output')\n]\n</code></pre> <p>what we'll get now is the pairs of raw, observed city and cleaned deduplicated city.</p> <pre><code>[\n    City(text='Chicago', inferred_city='Chicago'),\n    City(text='The Windy City', inferred_city='Chicago'),\n    City(text='New York City', inferred_city='New York City'),\n    City(text='The Big Apple', inferred_city='New York City'),\n    City(text='SF', inferred_city='San Francisco'),\n    City(text='San Fran', inferred_city='San Francisco'),\n    City(text='San Francisco', inferred_city='San Francisco')\n]\n</code></pre> <p>So, we've seen that deduplicating data with a Large Language Model is fairly straightforward in a customizable way using Marvin. If you want the entire content of the cells above in  one place, you can copy the cell below.</p> Copy <pre><code>import openai\nimport json\nimport pydantic\nfrom marvin import prompt_fn\n\nclass City(pydantic.BaseModel):\n    '''\n        A model to represent a city.\n    '''\n\n    text: str = pydantic.Field(\n        description = 'The city name as it appears'\n    )\n\n    inferred_city: str = pydantic.Field(\n            description = 'The inferred and normalized city name.'\n        )\n\n@prompt_fn\ndef get_cities(text: str) -&gt; list[City]:\n    '''\n        Expertly deduce and infer all cities from the follwing text: {{ text }}\n    '''\n\nresponse = openai.ChatCompletion.create(\n    api_key = 'YOUR OPENAI KEY',\n    model = 'gpt-3.5-turbo',\n    temperature = 0,\n    **get_cities(\n        (\n            \"Chicago, The Windy City, New York City, \"\n            \"The Big Apple, SF, San Fran, San Francisco.\"\n        )\n    )\n)\n\n[\n    City.parse_obj(city) \n    for city in \n    json.loads(\n        response.choices[0].message.function_call.arguments\n    ).get('output')\n]\n</code></pre>"},{"location":"examples/extraction_api/","title":"Basic Extraction API","text":"<p>With Marvin, you can easily build a production-grade application or data pipeline to extract structured data from unstructured text.  In this example, we'll show how to </p> <ul> <li>Write an AI-powered Function.</li> <li>Build a production-ready API.</li> </ul>"},{"location":"examples/extraction_api/#writing-an-ai-powered-function","title":"Writing an AI-powered Function.","text":"<p>Example</p> <p>Marvin translates your Python code into English, passes that to an Large Language Model,  and parses its response. It uses AI to evaluate your function, no code required.</p> <p>Let's build a function that extracts a person's first_name, last_name, and age  from a paragraph of text. This is typically a pretty daunting task for a machine learning savant, much less your average engineer. Marvin lets you accomplish this by write code  the way you normally would, no PhD required.</p> <p>We'll simply write a Python function, tell it that we expect a <code>text</code> input and that it'll  output a <code>dict</code>, or dictionary. With Marvin, we'll use <code>ai_fn</code> and decorate his function.  When we do, this function will use AI to get its answer.</p> <p><pre><code>from marvin import ai_fn, settings\nfrom typing import Any\n\nsettings.openai.api_key = 'API_KEY' \n\n@ai_fn\ndef extract_person(text: str) -&gt; dict:\n    '''\n        Correctly infers a persons `birth_year`, `first_name` and `last_name`\n        from the passed `text`. \n    '''\n</code></pre> This function can now be run! When we test it out, we get great results.</p> Results <pre><code>extract_person('My name is Peter Parker, and I was born when Clinton was first elected')\n\n# returns {'first_name': 'Peter', 'last_name': 'Parker', 'birth_year': 1992}\n</code></pre>"},{"location":"examples/extraction_api/#build-a-production-ready-api","title":"Build a production-ready API.","text":"<p>In the following example, we will demonstrate how to deploy the AI function we just created as an API using FastAPI. FastAPI is a powerful tool that allows us to easily turn our AI function into a fully-fledged API. This API can then be used by anyone to send a POST request to our <code>/extract_person/</code> endpoint and get the structured data they need. Let's see how this can be done.</p> <p>Example</p> <p>Now that we have our AI function, let's deploy it as an API using FastAPI. FastAPI is a modern, fast (high-performance), web framework for building APIs.</p> <pre><code>from fastapi import FastAPI\nfrom marvin import ai_fn, settings\n\napp = FastAPI()\n\nsettings.openai.api_key = 'API_KEY'\n\n@app.post(\"/extract_person/\")\n@ai_fn\ndef extract_person(text: str) -&gt; dict:\n    '''\n        Correctly infers a persons `birth_year`, `first_name` and `last_name`\n        from the passed `text`. \n    '''\n</code></pre> <p>With just a few lines of code, we've turned our AI function into a fully-fledged API. Now, anyone can send a POST request to our <code>/extract_person/</code> endpoint and get the structured data they need.</p> API Deployment <p><pre><code>import uvicorn\nimport asyncio\n\nconfig = uvicorn.Config(app)\nserver = uvicorn.Server(config)\nasyncio.run(server.serve())\n</code></pre> Now, you can navigate to localhost:8000/docs to interact with your API.</p> Making Requests <p><pre><code>import requests\n\ndata = {\"text\": \"My name is Peter Parker, and I was born when Clinton was first elected\"}\nresponse = requests.post(\"http://localhost:8000/extract_person/\", json=data)\nprint(response.json())\n\n# returns {'first_name': 'Peter', 'last_name': 'Parker', 'birth_year': 1992}\n</code></pre> This will send a POST request to the <code>/extract_person/</code> endpoint with the provided text and print the response.</p>"},{"location":"examples/github_digest/","title":"GitHub Digest","text":"<p>A fun example covering a few practical patterns is to create an AI digest of GitHub activity for a given repo.</p> <p>If you've spent some time messing with AI tools in the Python ecosystem lately, you're probably familiar with Jinja2. Jinja pairs really nicely with LLMs, because you can structure the template in a way that either makes it easy for the LLM to fill in the blanks, or makes it easy for you to fill in the blanks with traditional software and then pass the rendered template to the LLM as a prompt.</p> <p>Here's an example of the latter:</p>"},{"location":"examples/github_digest/#writing-an-epic-about-the-days-events-in-prefecthqprefect","title":"Writing an epic about the day's events in <code>PrefectHQ/prefect</code>","text":"<p>The AI part is pretty much English:</p> <pre><code>@ai_fn(\n    instructions=\"You are a witty and subtle orator. Speak to us of the day's events.\"\n)\ndef summarize_digest(markdown_digest: str) -&gt; str:\n    \"\"\"Given a markdown digest of GitHub activity, create a Story that is\n    informative, entertaining, and epic in proportion to the day's events -\n    an empty day should be handled with a short sarcastic quip about humans\n    and their laziness.\n\n    The story should capture collective efforts of the project.\n    Each contributor plays a role in this story, their actions\n    (issues raised, PRs opened, commits merged) shaping the events of the day.\n\n    The narrative should highlight key contributors and their deeds, drawing upon the\n    details in the digest to create a compelling and engaging tale of the day's events.\n    A dry pun or 2 are encouraged.\n\n    Usernames should be markdown links to the contributor's GitHub profile.\n\n    The story should begin with a short pithy welcome to the reader and have\n    a very short, summarizing title.\n    \"\"\" # noqa: E501 (to make Ruff happy)\n</code></pre> <p>... and the rest is just... Python?</p> <pre><code>import os\nimport inspect\nfrom datetime import date, datetime, timedelta\n\nfrom marvin import ai_fn\n\nfrom my_helpers import (\n    fetch_contributor_data,\n    post_slack_message,\n    YOUR_JINJA_TEMPLATE,\n)\n\nasync def daily_github_digest(\n    owner: str = \"PrefectHQ\",\n    repo: str = \"marvin\",\n    slack_channel: str = \"ai-tools\",\n    gh_token_env_var: str = \"GITHUB_PAT\",\n):\n    since = datetime.utcnow() - timedelta(days=1)\n\n    data = await fetch_contributor_data(\n        token=os.getenv(gh_token_env_var), # load from your secrets manager\n        owner=owner,\n        repo=repo,\n        since=since,\n    )\n\n    markdown_digest = YOUR_JINJA_TEMPLATE.render(\n        today=date.today(),\n        owner=owner,\n        repo=repo,\n        contributors_activity=data,\n    )\n\n    epic_story = summarize_digest(markdown_digest)\n\n    await post_slack_message(\n        message=epic_story,\n        channel=slack_channel,\n    )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(daily_github_digest(owner=\"PrefectHQ\", repo=\"prefect\"))\n</code></pre> <p>Tip</p> <p>I brought some helpers with me to make this easier:</p> <ul> <li><code>fetch_contributor_data</code> uses the GitHub API to get a list of contributors and their activity</li> <li><code>post_slack_message</code> uses the Slack API to post a message to a channel</li> <li><code>YOUR_JINJA_TEMPLATE</code> is a Jinja template that you can use to render the digest</li> </ul> <p>Find my helpers here</p> <p>Here's a sample output from August 16th 2023 on the <code>PrefectHQ/prefect</code> repo: <pre><code>Greetings, wanderer! Sit, rest your feet, and allow me to regale you with the\nheroic deeds of the PrefectHQ/prefect repository on this fateful day, the 16th\nof August, 2023.\n\nOur tale begins with the indefatigable jakekaplan, who single-handedly opened \ntwo perceptive Pull Requests: flow run viz v2 and don't run tasks draft-. Not\nsatisfied, he also valiantly merged five commits, clearing the path for others\nto tread. Meanwhile, cicdw, desertaxle, and serinamarie, each merged a single,\nbut significant commit, contributing their mite to the collective effort.\n\nIn the realm of PRs, prefectcboyd, WillRaphaelson, and bunchesofdonald all \nunfurled their banners, each opening a PR of their own, like pioneers staking\ntheir claim on the wild frontiers of code.\n\nIn the grand theatre of software development, where victory is measured in merged\ncommits and opened PRs, these individuals stood tall, their actions resonating \nthrough the corridors of GitHub. Their names are etched onto the scroll of this \nday, their deeds a testament to their commitment.\n\nSo ends the telling of this day's events. Until our paths cross again, wanderer, \nmay your code compile and your tests always pass!\n</code></pre></p>"},{"location":"examples/github_digest/#full-example","title":"Full example:","text":"<p>The full example, with improvements like caching and observability, can be found here.</p>"},{"location":"examples/slackbot/","title":"Build a Slack bot with Marvin","text":""},{"location":"examples/slackbot/#slack-setup","title":"Slack setup","text":"<p>Get a Slack app token from Slack API and add it to your <code>.env</code> file:</p> <pre><code>MARVIN_SLACK_API_TOKEN=your-slack-bot-token\n</code></pre> <p>Choosing scopes</p> <p>You can choose the scopes you need for your bot in the OAuth &amp; Permissions section of your Slack app.</p>"},{"location":"examples/slackbot/#building-the-bot","title":"Building the bot","text":""},{"location":"examples/slackbot/#define-a-message-handler","title":"Define a message handler","text":"<p><pre><code>import asyncio\nfrom fastapi import HTTPException\n\nasync def handle_message(payload: dict) -&gt; dict:\n    event_type = payload.get(\"type\", \"\")\n\n    if event_type == \"url_verification\":\n        return {\"challenge\": payload.get(\"challenge\", \"\")}\n    elif event_type != \"event_callback\":\n        raise HTTPException(status_code=400, detail=\"Invalid event type\")\n\n    asyncio.create_task(generate_ai_response(payload))\n\n    return {\"status\": \"ok\"}\n</code></pre> Here, we define a simple python function to handle Slack events and return a response. We run our interesting logic in the background using <code>asyncio.create_task</code> to make sure we return <code>{\"status\": \"ok\"}</code> within 3 seconds, as required by Slack.</p>"},{"location":"examples/slackbot/#implement-the-ai-response","title":"Implement the AI response","text":"<p>I like to start with this basic structure, knowing that one way or another...</p> <pre><code>async def generate_ai_response(payload: dict) -&gt; str:\n    # somehow generate the ai responses\n    ...\n\n    # post the response to slack\n    _post_message(\n        messsage=some_message_ive_constructed,\n        channel=event.get(\"channel\", \"\"),\n        thread_ts=thread_ts,\n    )\n</code></pre> <p>... I need to take in a Slack app mention payload, generate a response, and post it back to Slack.</p>"},{"location":"examples/slackbot/#a-couple-considerations","title":"A couple considerations","text":"<ul> <li>do I want the bot to respond to users in a thread or in the channel?</li> <li>do I want the bot to have memory of previous messages? how so?</li> <li>what tools do I need to generate accurate responses for my users?</li> </ul> <p>In our case of the Prefect Community slackbot, we want:</p> <ul> <li>the bot to respond in a thread</li> <li>the bot to have memory of previous messages by slack thread</li> <li>the bot to have access to the internet, GitHub, embedded docs, a calculator, and have the ability to immediately save useful slack threads to Discourse for future reference by the community</li> </ul>"},{"location":"examples/slackbot/#implementation-of-generate_ai_response-for-the-prefect-community-slackbot","title":"Implementation of <code>generate_ai_response</code> for the Prefect Community Slackbot","text":"<p>Here we invoke a worker <code>Chatbot</code> that has the <code>tools</code> needed to generate an accurate and helpful response.</p> <pre><code>async def generate_ai_response(payload: dict) -&gt; str:\n    event = payload.get(\"event\", {})\n    channel_id = event.get(\"channel\", \"\")\n    message = event.get(\"text\", \"\")\n\n    bot_user_id = payload.get(\"authorizations\", [{}])[0].get(\"user_id\", \"\")\n\n    if match := re.search(SLACK_MENTION_REGEX, message):\n        thread_ts = event.get(\"thread_ts\", \"\")\n        ts = event.get(\"ts\", \"\")\n        thread = thread_ts or ts\n\n        mentioned_user_id = match.group(1)\n\n        if mentioned_user_id != bot_user_id:\n            get_logger().info(f\"Skipping message not meant for the bot: {message}\")\n            return\n\n        message = re.sub(SLACK_MENTION_REGEX, \"\", message).strip()\n        # `CACHE` is a TTL cache that stores a `History` object for each thread\n        history = CACHE.get(thread, History())\n\n        bot = choose_bot(payload=payload, history=history)\n\n        ai_message = await bot.run(input_text=message)\n\n        CACHE[thread] = deepcopy(\n            bot.history\n        )  # make a copy so we don't cache a reference to the history object\n\n        message_content = _clean(ai_message.content)\n\n        await post_slack_message(\n            message=message_content,\n            channel=channel_id,\n            thread_ts=thread,\n        )\n\n        return message_content\n</code></pre> <p>This is just an example</p> <p>Find my specific helpers here.</p> <p>Unlike previous version of <code>marvin</code>, we don't necessarily have a database full of historical messages to pull from for a thread-based history. Instead, we'll cache the histories in memory for the duration of the app's runtime. Thread history can / should be implemented in a more robust way for specific use cases.</p>"},{"location":"examples/slackbot/#attach-our-handler-to-a-deployable-aiapplication","title":"Attach our handler to a deployable <code>AIApplication</code>","text":"<p>All Marvin components are directly deployable as FastAPI applications - check it out: <pre><code>from chatbot import handle_message\nfrom marvin import AIApplication\nfrom marvin.deployment import Deployment\n\ndeployment = Deployment(\n    component=AIApplication(tools=[handle_message]),\n    app_kwargs={\n        \"title\": \"Marvin Slackbot\",\n        \"description\": \"A Slackbot powered by Marvin\",\n    },\n    uvicorn_kwargs={\n        \"port\": 4200,\n    },\n)\n\nif __name__ == \"__main__\":\n    deployment.serve()\n</code></pre></p> <p>Deployments</p> <p>Learn more about deployments here.</p> <p>Run this file with something like:</p> <pre><code>python slackbot.py\n</code></pre> <p>... and navigate to <code>http://localhost:4200/docs</code> to see your bot's docs.</p> <p>This is now an endpoint that can be used as a Slack event handler. You can use a tool like ngrok to expose your local server to the internet and use it as a Slack event handler.</p>"},{"location":"examples/slackbot/#building-an-image","title":"Building an image","text":"<p>Based on this example, one could write a <code>Dockerfile</code> to build a deployable image:</p> <p><pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY . /app\n\nRUN python -m venv venv\nENV VIRTUAL_ENV=/app/venv\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\nRUN apt-get update &amp;&amp; apt-get install -y git\n\nRUN pip install \".[slackbot,ddg]\"\n\nEXPOSE 4200\n\nCMD [\"python\", \"cookbook/slackbot/start.py\"]\n</code></pre> Note that we're installing the <code>slackbot</code> and <code>ddg</code> extras here, which are required for tools used by the worker bot defined in this example's <code>cookbook/slackbot/start.py</code> file.</p>"},{"location":"examples/slackbot/#find-the-whole-example-here","title":"Find the whole example here.","text":""},{"location":"help/legacy_docs/","title":"Viewing legacy documentation","text":""},{"location":"help/legacy_docs/#prerequisites","title":"Prerequisites","text":"<ul> <li>Git</li> <li><code>python3</code></li> <li>pip</li> </ul>"},{"location":"help/legacy_docs/#automated-script-for-legacy-documentation","title":"Automated Script for Legacy Documentation","text":"<p>To build and view the docs for a specific version of Marvin, you can use this script.</p> <p>You can either clone the Marvin repo and run the script locally, or copy the script and run it directly in your terminal after making it executable: <pre><code># unix\nchmod +x scripts/serve_legacy_docs\n\n# run the script (default version is v1.5.6)\n./scripts/serve_legacy_docs\n\n# optionally, specify a version\n./scripts/serve_legacy_docs v1.5.3\n</code></pre></p>"},{"location":"help/legacy_docs/#manual-steps","title":"Manual Steps","text":"<p>If you prefer to manually perform the steps or need to tailor them for your specific operating system, follow these instructions:</p> <ol> <li> <p>Clone the Repository    Clone the Marvin repository using Git:    <pre><code>git clone https://github.com/PrefectHQ/marvin.git\ncd marvin\n</code></pre></p> </li> <li> <p>Checkout the Specific Tag    Checkout the tag for the version you are interested in. Replace <code>v1.5.6</code> with the desired version tag:    <pre><code>git fetch --tags\ngit checkout tags/v1.5.6\n</code></pre></p> </li> <li> <p>Create a Virtual Environment    Create and activate a virtual environment to isolate the dependency installation:    <pre><code>python3 -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n</code></pre></p> </li> <li> <p>Install Dependencies    Install the necessary dependencies for the documentation:    <pre><code>pip install -e \".[dev,docs]\"\n</code></pre></p> </li> <li> <p>Serve the Documentation Locally    Use <code>mkdocs</code> to serve the documentation:    <pre><code>mkdocs serve\n</code></pre>    This will start a local server. View the documentation by navigating to <code>http://localhost:8000</code> in your web browser.</p> </li> <li> <p>Exit Virtual Environment    Once finished, you can exit the virtual environment:    <pre><code>deactivate\n</code></pre></p> </li> </ol> <p>Optionally, you can remove the virtual environment folder:    <pre><code>rm -rf venv\n</code></pre></p>"},{"location":"prompting/prompt_function/","title":"Prompt function","text":"<p>Marvin puts the engineering in prompt engineering. We expose a low-level <code>prompt_fn</code> decorator that lets you write prompts as functions. This lets you build fully type-hinted prompts that other engineers can introspect, version, and test.</p> <p>This is the easiest way to use Azure / OpenAI's function calling API.</p>"},{"location":"prompting/prompt_function/#basic-use","title":"Basic Use","text":""},{"location":"prompting/prompt_function/#type-hinting","title":"Type Hinting","text":"<p>Example</p> <p>Marvin translates your Python code into English. We'll simply write a Python function,  tell it that we expect an integer input <code>n</code> input and that it'll  output <code>list[str]</code>, or a list of strings. With Marvin, we'll use <code>prompt_fn</code> and decorate this function.  When we do, this function can be cast to a payload that can be send to an LLM.</p> <p><pre><code>from marvin.prompts import prompt_fn\n\n@prompt_fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    '''Generates a list of {{ n }} {{ color }} fruits'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Generates a list of 3 blue fruits\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"output\": {\n                \"title\": \"Output\",\n                \"type\": \"array\",\n                \"items\": {\n                \"type\": \"string\"\n                }\n            }\n            },\n            \"required\": [\n            \"output\"\n            ]\n        },\n        \"name\": \"Output\",\n        \"description\": \"\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"Output\"\n    }\n    }\n</code></pre>"},{"location":"prompting/prompt_function/#advanced-use","title":"Advanced Use","text":""},{"location":"prompting/prompt_function/#use-with-pydantic","title":"Use with Pydantic","text":"<p>Example</p> <p>Marvin supports type-hinting with Pydantic, so your return annotation can be a more complex data-model.</p> <p><pre><code>from marvin.prompts import prompt_fn\nfrom pydantic import BaseModel\n\nclass Fruit(BaseModel):\n    color: str\n\n@prompt_fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[Fruit]:\n    '''Generates a list of {{ n }} {{ color }} fruits'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Generates a list of 3 blue fruits\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"output\": {\n                \"title\": \"Output\",\n                \"type\": \"array\",\n                \"items\": {\n                \"$ref\": \"#/definitions/Fruit\"\n                }\n            }\n            },\n            \"required\": [\n            \"output\"\n            ],\n            \"definitions\": {\n            \"Fruit\": {\n                \"title\": \"Fruit\",\n                \"type\": \"object\",\n                \"properties\": {\n                \"color\": {\n                    \"title\": \"Color\",\n                    \"type\": \"string\"\n                }\n                },\n                \"required\": [\n                \"color\"\n                ]\n            }\n            }\n        },\n        \"name\": \"FruitList\",\n        \"description\": \"\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"FruitList\"\n    }\n    }\n</code></pre>"},{"location":"prompting/prompt_function/#full-customization","title":"Full Customization","text":"<p>Example</p> <p>Marvin supports full customization of every element of your prompts. You can customize the  name, description, and field_names of your <code>response_model</code>. </p> <p>Say we want to change the task to generate in Swedish.</p> <p><pre><code>from marvin.prompts import prompt_fn\nfrom pydantic import BaseModel\n\nclass Fruit(BaseModel):\n    color: str\n\n@prompt_fn(\n    response_model_name = 'Fruktlista', \n    response_model_description = 'A list of fruits in Swedish',\n    response_model_field_name = 'Frukt'\n)\ndef list_fruits(n: int, color: str = 'red') -&gt; list[Fruit]:\n    '''Generates a list of {{ n }} {{ color }} fruits'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Generates a list of 3 blue fruits\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"Frukt\": {\n                \"title\": \"Frukt\",\n                \"type\": \"array\",\n                \"items\": {\n                \"$ref\": \"#/definitions/Fruit\"\n                }\n            }\n            },\n            \"required\": [\n            \"Frukt\"\n            ],\n            \"definitions\": {\n            \"Fruit\": {\n                \"title\": \"Fruit\",\n                \"type\": \"object\",\n                \"properties\": {\n                \"color\": {\n                    \"title\": \"Color\",\n                    \"type\": \"string\"\n                }\n                },\n                \"required\": [\n                \"color\"\n                ]\n            }\n            }\n        },\n        \"name\": \"Fruktlista\",\n        \"description\": \"A list of fruits in Swedish\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"Fruktlista\"\n    }\n    }\n</code></pre>"},{"location":"prompting/prompt_function/#referencing-globals","title":"Referencing Globals","text":"<p>Example</p> <p>Marvin passes the name of your response model to the prompt for you to reference as a convenience.</p> <p>Say we want to change the task to generate in Swedish.</p> <p><pre><code>from marvin.prompts import prompt_fn\nfrom pydantic import BaseModel\n\nclass Fruit(BaseModel):\n    color: str\n\n@prompt_fn(response_model_name = 'Fruits')\ndef list_fruits(n: int, color: str = 'red') -&gt; list[Fruit]:\n    '''Generates a list of {{ n }} {{ color }} {{ response_model.__name__.lower() }}'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n        \"messages\": [\n            {\n            \"role\": \"system\",\n            \"content\": \"Generates a list of 3 blue fruits\"\n            }\n        ],\n        \"functions\": [\n            {\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                \"output\": {\n                    \"title\": \"Output\",\n                    \"type\": \"array\",\n                    \"items\": {\n                    \"$ref\": \"#/definitions/Fruit\"\n                    }\n                }\n                },\n                \"required\": [\n                \"output\"\n                ],\n                \"definitions\": {\n                \"Fruit\": {\n                    \"title\": \"Fruit\",\n                    \"type\": \"object\",\n                    \"properties\": {\n                    \"color\": {\n                        \"title\": \"Color\",\n                        \"type\": \"string\"\n                    }\n                    },\n                    \"required\": [\n                    \"color\"\n                    ]\n                }\n                }\n            },\n            \"name\": \"Fruits\",\n            \"description\": \"\"\n            }\n        ],\n        \"function_call\": {\n            \"name\": \"Fruits\"\n        }\n        }\n</code></pre>"},{"location":"prompting/prompt_function/#contexts","title":"Contexts","text":"<p>Example</p> <p>Marvin supports full passing context dictionaries to your prompt's rendering environment.</p> <p>Say we want to list 'seasonal' fruits. We'll pass the datetime.</p> <pre><code>from marvin.prompts import prompt_fn\nfrom datetime import date\n\n@prompt_fn(ctx = {'today': date.today()})\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    ''' \n    Generates a list of {{ n }} {{ color }} fruits in season.\n        - The date is {{ today }}\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n        \"messages\": [\n            {\n            \"role\": \"system\",\n            \"content\": \"Generates a list of 3 blue fruits in season.\\n- The date is 2023-09-22\"\n            }\n        ],\n        \"functions\": [\n            {\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                \"output\": {\n                    \"title\": \"Output\",\n                    \"type\": \"array\",\n                    \"items\": {\n                    \"type\": \"string\"\n                    }\n                }\n                },\n                \"required\": [\n                \"output\"\n                ]\n            },\n            \"name\": \"Output\",\n            \"description\": \"\"\n            }\n        ],\n        \"function_call\": {\n            \"name\": \"Output\"\n        }\n        }\n</code></pre>"},{"location":"prompting/prompt_function/#multi-turn-prompts","title":"Multi-Turn Prompts","text":"<p>Example</p> <p>Marvin supports multi-turn conversations. If no role is specified, the whole block is assumed to be a system prompt. To override this default behavior, simply break into Human, System, Assistant turns. </p> <pre><code>from marvin.prompts import prompt_fn\nfrom datetime import date\n\n@prompt_fn(ctx = {'today': date.today()})\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    ''' \n    System: You generate a list of {{ n }} fruits in season.\n        - The date is {{ today }}\n\n    User: I want {{ color }} fruits only.\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n        \"messages\": [\n            {\n            \"role\": \"system\",\n            \"content\": \"You generate a list of 3 fruits in season.\\n- The date is 2023-09-22\"\n            },\n            {\n            \"role\": \"user\",\n            \"content\": \"I want blue fruits.\"\n            }\n        ],\n        \"functions\": [\n            {\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                \"output\": {\n                    \"title\": \"Output\",\n                    \"type\": \"array\",\n                    \"items\": {\n                    \"type\": \"string\"\n                    }\n                }\n                },\n                \"required\": [\n                \"output\"\n                ]\n            },\n            \"name\": \"Output\",\n            \"description\": \"\"\n            }\n        ],\n        \"function_call\": {\n            \"name\": \"Output\"\n        }\n        }\n</code></pre>"},{"location":"prompting/prompt_function/#use-cases","title":"Use Cases","text":""},{"location":"prompting/prompt_function/#classification","title":"Classification","text":"<p>Example</p> <p>Marvin supports multi-turn conversations. If no role is specified, the whole block is assumed to be a system prompt. To override this default behavior, simply break into Human, System, Assistant turns. </p> <pre><code>from marvin.prompts import prompt_fn\nfrom typing import Optional\nfrom enum import Enum\n\nclass Food(Enum):\n    '''\n        Food classes\n    '''\n    FRUIT = 'Fruit'\n    VEGETABLE = 'Vegetable'\n\n@prompt_fn\ndef classify_fruits(food: str) -&gt; Food:\n    ''' \n        Expertly determines the class label of {{ food }}.\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>classify_fruits('tomato')</code> <pre><code>{\n\"messages\": [\n    {\n    \"role\": \"system\",\n    \"content\": \"Expertly determines the class label of tomato.\"\n    }\n],\n\"functions\": [\n    {\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n        \"output\": {\n            \"$ref\": \"#/definitions/Food\"\n        }\n        },\n        \"required\": [\n        \"output\"\n        ],\n        \"definitions\": {\n        \"Food\": {\n            \"title\": \"Food\",\n            \"description\": \"Food classes\",\n            \"enum\": [\n            \"Fruit\",\n            \"Vegetable\"\n            ]\n        }\n        }\n    },\n    \"name\": \"Output\",\n    \"description\": \"\"\n    }\n],\n\"function_call\": {\n    \"name\": \"Output\"\n}\n}   \n</code></pre>"},{"location":"prompting/prompt_function/#entity-extraction","title":"Entity Extraction","text":"<p>Example</p> <p>In this example, Marvin is configured to perform entity extraction on a list of fruits mentioned in a text string. The function <code>extract_fruits</code> identifies and extracts fruit entities based on the input text. These entities are then returned as a list of Pydantic models.</p> <pre><code>from marvin.prompts import prompt_fn\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass FruitEntity(BaseModel):\n    '''\n        Extracted Fruit Entities\n    '''\n    name: str\n    color: str\n\n@prompt_fn\ndef extract_fruits(text: str) -&gt; List[FruitEntity]:\n    ''' \n        Extracts fruit entities from the given text: {{ text }}.\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> <code>extract_fruits('There are red apples and yellow bananas.')</code> <pre><code>{\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Extracts fruit entities from the given text: There are red apples and yellow bananas..\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"output\": {\n                \"title\": \"Output\",\n                \"type\": \"array\",\n                \"items\": {\n                \"$ref\": \"#/definitions/FruitEntity\"\n                }\n            }\n            },\n            \"required\": [\n            \"output\"\n            ],\n            \"definitions\": {\n            \"FruitEntity\": {\n                \"title\": \"FruitEntity\",\n                \"description\": \"Extracted Fruit Entities\",\n                \"type\": \"object\",\n                \"properties\": {\n                \"name\": {\n                    \"title\": \"Name\",\n                    \"type\": \"string\"\n                },\n                \"color\": {\n                    \"title\": \"Color\",\n                    \"type\": \"string\"\n                }\n                },\n                \"required\": [\n                \"name\",\n                \"color\"\n                ]\n            }\n            }\n        },\n        \"name\": \"Output\",\n        \"description\": \"\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"Output\"\n    }\n    }\n</code></pre>"},{"location":"utilities/chat_completion/","title":"Chat completion","text":"<p>In Marvin, each supported Large Language Model can be accessed with one common API. This means that  you can easily switch between providers without having to change your code. We have anchored our API  to mirror that of OpenAI's Python SDK. </p> <p>In plain English.</p> <ul> <li>A drop-in replacement for OpenAI's ChatCompletion, with sensible superpowers.</li> <li>You can use Anthropic and other Large Language Models as if you were using OpenAI.</li> </ul>"},{"location":"utilities/chat_completion/#basic-use","title":"Basic Use","text":"<p>Using a single interface to multiple models helps reduce boilerplate code and translation. In  the current era of building with different LLM providers, developers often need to rewrite their code just to use a new model. With Marvin you can simply import ChatCompletion and  specify a model name.</p> <p>Example: Specifying a Model</p> <p>We first past the API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import ChatCompletion\nimport os\n\nos.environ['OPENAI_API_KEY'] = 'openai_private_key'\nos.environ['ANTHROPIC_API_KEY'] = 'anthropic_private_key'\n</code></pre> ChatCompletion recognizes the model name and correctly routes it to the correct provider. So  you can simply pass 'gpt-3.5-turbo' or 'claude-2' and it just works.</p> <p><pre><code># Set up a dummy list of messages.\nmessages = [{'role': 'user', 'content': 'Hey! How are you?'}]\n\n# Call gpt-3.5-turbo simply by specifying it inside of ChatCompletion.\nopenai = ChatCompletion('gpt-3.5-turbo').create(messages = messages)\n\n# Call claude-2 simply by specifying it inside of ChatCompletion.\nanthropic = ChatCompletion('claude-2').create(messages = messages)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>print(openai.choices[0].message.content)\n# Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you?\n\nprint(anthropic.choices[0].message.content)\n# I'm doing well, thanks for asking!\n</code></pre> <p>You can set more than just the model and provider as a default value. Any keyword arguments passed to ChatCompletion will be persisted and passed to subsequent requests.</p> <p>Example: Frozen Model Facets</p> <p><pre><code># Create system messages or conversation history to seed.\nsystem_messages = [{'role': 'system', 'content': 'You talk like a pirate'}]\n\n# Instatiate gpt-3.5.turbo with the previous system_message. \nopenai_pirate = ChatCompletion('gpt-3.5.turbo', messages = system_messages)\n\n# Call the instance with create. \nopenai_pirate.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Hey! How are you?'\n    }]\n)\n</code></pre> For functions and messages, this will concatenate the frozen and passed arguments. All other passed keyword arguments will overwrite the default settings.</p> <pre><code>print(openai_pirate.choices[0].message.content)\n# Arrr, matey! I be doin' well on this fine day. How be ye farein'?\n</code></pre> <p>Replacing OpenAI's ChatCompletion.</p> <p>ChatCompletion is designed to be a drop-in replacement for OpenAI's ChatCompletion.  Just import openai from marvin or, equivalently, ChatCompletion from marvin.openai. </p> <pre><code>from marvin import openai\n\n\nopenai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Hey! How are you?'\n    }]\n)\n</code></pre>"},{"location":"utilities/chat_completion/#advanced-use","title":"Advanced Use","text":""},{"location":"utilities/chat_completion/#response-model","title":"Response Model","text":"<p>With Marvin, you can get structured outputs from model providers by passing a response type. This lets developers write prompts with Python objects, which are easier to develop, version, and test than language.</p> <p>In plain English.</p> <p>You can specify a type, struct, or data model to ChatCompletion, and Marvin will ensure the model's response adheres to that type.</p> <p>Let's consider two examples.</p> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass CoffeeOrder(BaseModel):\n    size: Literal['small', 'medium', 'large']\n    milk: Literal['soy', 'oat', 'dairy']\n    with_sugar: bool = False\n\n\nresponse = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Can I get a small soymilk latte?'\n    }],\n    response_model = CoffeeOrder\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# CoffeeOrder(size='small', milk='soy', with_sugar=False)\n</code></pre> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass Translation(BaseModel):\n    spanish: str\n    french: str\n    swedish: str\n\n\nresponse = openai.ChatCompletion.create(\n    messages = [\n    {\n        'role': 'system',\n        'content': 'You translate user messages into other languages.'\n    },\n    {\n        'role': 'user',\n        'content': 'Can I get a small soymilk latte?'\n    }],\n    response_model = Translation\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# Translation(\n#   spanish='\u00bfPuedo conseguir un caf\u00e9 con leche de soja peque\u00f1o?', \n#   french='Puis-je avoir un petit latte au lait de soja ?', \n#   swedish='Kan jag f\u00e5 en liten sojamj\u00f6lklatt\u00e9?'\n# )\n</code></pre>"},{"location":"utilities/chat_completion/#function-calling","title":"Function Calling","text":"<p>ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate. </p> <p>Marvin lets you pass your choice of JSON Schema or Python functions directly to ChatCompletion. It does the right thing.</p> <p>In plain English.</p> <p>You can pass regular Python functions to ChatCompletion, and Marvin will take care of serialization of that function using <code>Pydantic</code> in a way you can customize.</p> <p>Let's consider an example.</p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We have the usual annuity formula from accounting, which we can write deterministically. We wouldn't expect an LLM to be able to both handle semantic parsing and math in one fell swoop, so we want to pass it a hardcoded function so it's only task is to compute its arguments. <pre><code>from marvin import openai\nfrom pydantic import BaseModel\n\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'What if I put it $100 every month for 60 months at 12%?'\n    }],\n    functions = [annuity_present_value]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 4495.5}\n</code></pre> <p>In the case where several functions are passed. It does the right thing. </p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We want to give it another tool from  accounting 101: the ability to compute compound interest. It'll now have to tools to choose from: <pre><code>from marvin import openai\nfrom pydantic import BaseModel\n\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n\ndef compound_interest(P: float, r: float, t: float, n: int) -&gt; float:\n    \"\"\"\n    This function calculates and returns the total amount of money \n    accumulated after n times compounding interest per year at an annual \n    interest rate of r for a period of t years on an initial amount of P.\n    \"\"\"\n    A = P * (1 + r/n)**(n*t)\n    return round(A,2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'If I have $5000 in my account today and leave it in for 5 years at 12%?'\n    }],\n    functions = [annuity_present_value, compound_interest]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n\n# {'role': 'function', 'name': 'compound_interest', 'content': 8811.71}\n</code></pre> <p>Of course, we if ask if about repeated deposits, it'll correctly call the right function.</p> <pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'What if I put in $50/mo for 60 months at 12%?'\n    }],\n    functions = [annuity_present_value, compound_interest]\n)\n\nresponse.call_function()\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 2247.75}\n</code></pre>"},{"location":"utilities/chat_completion/#chaining","title":"Chaining","text":"<p>Above we saw how ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate.</p> <p>Often we want to take the output of a function call and pass it back to an LLM so that it can either call a new function or summarize the results of what we've computed for it. This agentic pattern is easily enabled with Marvin. </p> <p>Rather than write while- and for- loops for you, we've made ChatCompletion a context manager. This lets you maintain a state of a conversation that you can send and receive messages from. You have complete control over the internal logic.</p> <p>In plain English.</p> <p>You can have a conversation with an LLM, exposing functions for it to use in service of your request.  Marvin maintains state to make it easier to maintain and observe this conversation.</p> <p>Let's consider an example.</p> <p>Example: Chaining</p> <p>Let's build a simple arithmetic bot. We'll empower with arithmetic operations, like <code>add</code> and <code>divide</code>. We'll seed it with an arithmetic question.</p> <p><pre><code>from marvin import openai\nopenai.api_key = 'secret_key'\n\ndef divide(x: float, y: float) -&gt; str:\n    '''Divides x and y'''\n    return str(x/y)\n\ndef add(x: int, y: int) -&gt; str:\n    '''Adds x and y'''\n    return str(x+y)\n\nwith openai.ChatCompletion(functions = [add, divide]) as conversation:\n\n    # Start off with an external question / prompt. \n    prompt = 'What is 4124124 + 424242 divided by 48124?'\n\n    # Initialize the conversation with a prompt from the user. \n    conversation.send(messages = [{'role': 'user', 'content': prompt}])\n\n    # While the most recent turn has a function call, evaluate it. \n    while conversation.last_response.has_function_call():\n\n        # Send the most recent function call to the conversation. \n        conversation.send(messages = [\n            conversation.last_response.call_function() \n        ])\n</code></pre> The context manager, which we've called conversation (you can call it whatever you want), holds every turn of the conversation which we can inspect. </p> <pre><code>conversation.last_response.choices[0].message.content\n\n# The result of adding 4124124 and 424242 is 4548366. When this result is divided by 48124, \n# the answer is approximately 94.51346521486161.\n</code></pre> <p>If we want to see the entire state, every <code>[request, response]</code> pair is held in the conversation's  <code>turns</code>. <pre><code>[response.choices[0].message for response in conversation.turns]\n\n# [&lt;OpenAIObject at 0x120667c50&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"add\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4124124,\\n  \\\"y\\\": 424242\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4830&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"divide\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4548366,\\n  \\\"y\\\": 48124\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4b90&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": \"The result of adding 4124124 and 424242 is 4548366. \n#             When this result is divided by 48124, the answer is \n#             approximately 94.51346521486161.\"\n# }]\n</code></pre></p>"},{"location":"welcome/installation/","title":"Installation","text":""},{"location":"welcome/installation/#basic-installation","title":"Basic Installation","text":"<p>You can install Marvin with <code>pip</code> (note that Marvin requires Python 3.9+):</p> <pre><code>pip install marvin\n</code></pre> <p>To verify your installation, run <code>marvin --help</code> in your terminal. </p> <p>You can upgrade to the latest released version at any time:</p> <pre><code>pip install marvin -U\n</code></pre> <p>Breaking changes in 1.0</p> <p>Please note that Marvin 1.0 introduces a number of breaking changes and is not compatible with Marvin 0.X.</p>"},{"location":"welcome/installation/#adding-optional-dependencies","title":"Adding Optional Dependencies","text":"<p>Marvin's base install is designed to be as lightweight as possible, with minimal dependencies. To use functionality that interacts with other services, install Marvin with any required optional dependencies. For example, to use Anthropic models, install Marvin with the optional Anthropic provider:</p> <pre><code>pip install 'marvin[anthropic]'\n</code></pre>"},{"location":"welcome/installation/#installing-for-development","title":"Installing for Development","text":"<p>See the contributing docs for instructions on installing Marvin for development.</p>"},{"location":"welcome/overview/","title":"The Marvin Docs","text":"<p>Marvin is a collection of powerful building blocks that are designed to be incrementally adopted. This means that you should be able to use any piece of Marvin without needing to learn too much Marvin-specific information.</p> <p>For most users, this means they'll dive in with the highest-level abstractions, like AI Models and AI Functions, in order to immediately put Marvin to work. However, Marvin's documentation is organized to start with the most basic, low-level components in order to build up a cohesive explanation of how the higher-level objects work.</p>"},{"location":"welcome/overview/#layout","title":"Layout","text":""},{"location":"welcome/overview/#configuration","title":"Configuration","text":"<p>Details on setting up Marvin and configuring various aspects of its behavior, including LLM providers.</p>"},{"location":"welcome/overview/#ai-components","title":"AI Components","text":"<p>Documentation for Marvin's \"AI Building Blocks:\" familiar, Pythonic interfaces to AI-powered functionality.</p> <ul> <li>AI Model: a drop-in replacement for Pydantic's <code>BaseModel</code> that can be instantiated from unstructured text</li> <li>AI Classifier: a drop-in replacement for Python's enum that uses an LLM to select the most appopriate value</li> <li>AI Function: a function that uses an LLM to predict its output, making it ideal for NLP tasks</li> <li>AI Application: a stateful application intended for interactive use over multiple invocations</li> </ul>"},{"location":"welcome/overview/#openai-api-utilities","title":"OpenAI API Utilities","text":"<p>Marvin exposes a simple API for building prompts and calling LLMs, designed to be a drop-in replacement for OpenAI's Python SDK (but with support for other providers).</p>"},{"location":"welcome/overview/#examples","title":"Examples","text":"<p>Finally, for deeper dives into how to use Marvin, check out our examples like the Slackbot or GitHub Activity Digest.</p>"},{"location":"welcome/quickstart/","title":"Quickstart","text":"<p>After installing Marvin, the fastest way to get started is by using one of Marvin's high-level AI components. These components are designed to integrate AI into abstractions you already know well, creating the best possible opt-in developer experience.</p>"},{"location":"welcome/quickstart/#configure-llm-provider","title":"Configure LLM Provider","text":"<p>Marvin is a high-level interface for working with LLMs. In order to use it, you must configure an LLM provider. At this time, Marvin supports OpenAI's GPT-3.5 and GPT-4 models, Anthropic's Claude 1 and Claude 2 models, and the Azure OpenAI Service. The default model is OpenAI's <code>gpt-4</code>.</p> <p>To use the default model, provide an API key:</p> <pre><code>import marvin\n\n# to use an OpenAI model (if not specified, defaults to gpt-4)\nmarvin.settings.openai.api_key = YOUR_API_KEY\n</code></pre> <p>To use another provider or model, please see the configuration docs.</p>"},{"location":"welcome/quickstart/#ai-models","title":"AI Models","text":"<p>Marvin's most basic component is the AI Model, a drop-in replacement for Pydantic's <code>BaseModel</code>. AI Models can be instantiated from any string, making them ideal for structuring data, entity extraction, and synthetic data generation:</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str = Field(..., description=\"The two-letter state abbreviation\")\n\n\nLocation(\"The Big Apple\")\n</code></pre> <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"welcome/quickstart/#ai-classifiers","title":"AI Classifiers","text":"<p>AI Classifiers let you build multi-label classifiers with no code and no training data. Given user input, each classifier uses a clever logit bias trick to force an LLM to deductively choose the best option. It's bulletproof, cost-effective, and lets you build classifiers as quickly as you can write your classes.</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n\nAppRoute(\"update my name\")\n</code></pre> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"welcome/quickstart/#ai-functions","title":"AI Functions","text":"<p>AI Functions look like regular functions, but have no source code. Instead, an AI uses their description and inputs to generate their outputs, making them ideal for NLP applications like sentiment analysis.</p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment(text: str) -&gt; float:\n    \"\"\"\n    Given `text`, returns a number between 1 (positive) and -1 (negative)\n    indicating its sentiment score.\n    \"\"\"\n\n\nprint(\"Text 1:\", sentiment(\"I love working with Marvin!\"))\nprint(\"Text 2:\", sentiment(\"These examples could use some work...\"))\n</code></pre> <pre><code>Text 1: 0.8\nText 2: -0.2\n</code></pre> <p>Because AI functions are just like regular functions, you can quickly modify them for your needs. Here, we modify the above example to work with multiple strings at once:</p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n    \"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\n\n\nsentiment_list(\n    [\n        \"That was surprisingly easy!\",\n        \"Oh no, not again.\",\n    ]\n)\n</code></pre> <pre><code>[0.7, -0.5]\n</code></pre>"},{"location":"welcome/quickstart/#ai-applications","title":"AI Applications","text":"<p>AI Applications are the base class for interactive use cases. They are designed to be invoked one or more times, and automatically maintain three forms of state:</p> <ul> <li><code>state</code>: a structured application state</li> <li><code>plan</code>: high-level planning for the AI assistant to keep the application \"on-track\" across multiple invocations</li> <li><code>history</code>: a history of all LLM interactions</li> </ul> <p>AI Applications can be used to implement many \"classic\" LLM use cases, such as chatbots, tool-using agents, developer assistants, and more. In addition, thanks to their persistent state and planning, they can implement applications that don't have a traditional chat UX, such as a ToDo app. Here's an example:</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom marvin import AIApplication\n\n\n# create models to represent the state of our ToDo app\nclass ToDo(BaseModel):\n    title: str\n    description: str = None\n    due_date: datetime = None\n    done: bool = False\n\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n\n\n# create the app with an initial state and description\ntodo_app = AIApplication(\n    state=ToDoState(),\n    description=(\n        \"A simple todo app. Users will provide instructions for creating and updating\"\n        \" their todo lists.\"\n    ),\n)\n</code></pre> <p>Now we can invoke the app directly to add a to-do item. Note that the app understands that it is supposed to manipulate state, not just respond conversationally.</p> <pre><code># invoke the application by adding a todo\nresponse = todo_app(\"I need to go to the store tomorrow at 5pm\")\n\n\nprint(\n    f\"Response: {response.content}\\n\",\n)\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Sure! I've added a new task to your to-do list. You need to go to the store tomorrow at 5pm.\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": null,\n      \"due_date\": \"2023-07-12T17:00:00\",\n      \"done\": false\n    }\n  ]\n}\n</code></pre> <p>We can inform the app that we already finished the task, and it updates state appropriately</p> <pre><code># complete the task\nresponse = todo_app(\"I already went\")\n\n\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Great! I've marked the task \"Go to the store\" as completed. Is there anything else you need help with?\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": null,\n      \"due_date\": \"2023-07-12T17:00:00\",\n      \"done\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"welcome/what_is_marvin/","title":"Hello, Marvin!","text":"<pre><code>from marvin import ai_fn\n\n@ai_fn\ndef quote_marvin(topic: str) -&gt; str:\n    \"\"\"Quote Marvin the robot from Hitchhiker's Guide on a topic\"\"\"\n\nquote_marvin(topic=\"humans\") # \"I've seen it. It's rubbish.\"\n</code></pre> <p>Marvin is a lightweight AI engineering framework for building natural language interfaces that are reliable, scalable, and easy to trust.</p> <p>Sometimes the most challenging part of working with generative AI is remembering that it's not magic; it's software. It's new, it's nondeterministic, and it's incredibly powerful - but still software.</p> <p>Marvin's goal is to bring the best practices for building dependable, observable software to generative AI. As the team behind Prefect, which does something very similar for data engineers, we've poured years of open-source developer tool experience and lessons into Marvin's design.</p>"},{"location":"welcome/what_is_marvin/#core-components","title":"Core Components","text":"<p>\ud83e\udde9 AI Models for structuring text into type-safe schemas</p> <p>\ud83c\udff7\ufe0f AI Classifiers for bulletproof classification and routing</p> <p>\ud83e\ude84 AI Functions for complex business logic and transformations</p>"},{"location":"welcome/what_is_marvin/#ambient-ai","title":"Ambient AI","text":"<p>With Marvin, we\u2019re taking the first steps on a journey to deliver Ambient AI: omnipresent but unobtrusive autonomous routines that act as persistent translators for noisy, real-world data. Ambient AI makes unstructured data universally accessible to traditional software, allowing the entire software stack to embrace AI technology without interrupting the development workflow. Marvin brings simplicity and stability to AI engineering through abstractions that are reliable and easy to trust.</p> <p>Interested? Join our community!</p>"}]}